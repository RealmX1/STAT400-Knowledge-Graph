\chapter[Probability]{Probability}
\section{Sets, Experiment, and Probability}





To say that \textbf{an event $E$ has happened} means "we performed the experiment, and the outcome of this experiment was in the set $E$". 











 




 


Our goal is to assign probabilities or chance to events $E$ corresponding to an experiment. Since events and sample spaces are described as sets and subsets, we will first need to remind ourselves of how to work with these mathematical objects.  

\section[Set Theory]{Basic Set Theory}
We will need to understand some basics of sets and set operations. We define the operations along with the visual of the the operation using Venn Diagrams.



Let $A,B$ be two sets.\\






















We will often have to work with an infinite collection of sets, to do so, we will first need to label the sets using an "indexing set". Given a set $\Gamma$, a collection of sets indexed by $\Gamma$ will be denoted by $\{A_\alpha\}_{\alpha\in \Gamma}$. 
\\
A set is \textbf{countable} if there is a one-to-one correspondence with its elements, and the set of natural number $\mathbb{N}= \{1, 2, 3, 4, \dots\}$. 
\\

Let $\{A_i\}_{i\in\mathbb{N}}$ be a countable collection of sets. 








\subsection[Axioms of Probability]{Axioms of Probability}
Let $\mathcal{S}$ be the sample space of an experiment and let $E\subset \mathcal{S}$ be and event. We are interested in assigning a number to $E$ that quantifies the chance/probability that $E$ occurs. We will denote the probability of $E$ as $P(E)$.
\\
The mathematical procedure to assign probabilities to events is to define a function on the "event space". Assigning probabilities to every possible event can be tedious, and in some case it might not be possible. As a result, we will first need to identify a collection of events that we consider "interesting" and to which we want to assign probabilities. Such a collection of events is called a "sigma algebra". 


Note that there are multiple sigma algebras that can be associated to a sample space. The trivial sigma algebra is $\{\emptyset, \mathcal{S}\}$ - the smallest possible sigma algebra. The power set $\mathcal{P}(\mathcal{S})$ of $\mathcal{S}$ also satisfies the axioms to be a sigma algebra, and this is the largest possible sigma algebra associated to $\mathcal{S}$. All other sigma algebra are somewhere in between these two extreme examples. We can use sub-setting as a natural way to define a partial order on all possible sigma algebras. 
\\

A probability function assigns probabilities to events in the sigma algebra as follows.




Defining a probability function can often times be a tedious task, as the number of conditions needed to be checked grow exponentially (as a function of the sigma algebra). 
\\
When the sample space $\mathcal{S}$ is countable, it is possible to define a probability function on the power set of $\mathcal{S}$ using the following theorem.



Using the theorem above, we can assign probability functions experiments with countable sample space, by assigning probabilities just to the simple events. 



\section{Conditional Probability and Independence}
If the probability function attached to an experiments must be useful (say to draw some conclusions about events associated to the experiment), we will need to incorporate the features of the experiment as in our pursuit to calculate probability functions. 



Given two events $E$ and $F$ we can consider the following events:
\begin{enumerate}
    \item $E\cap F$: both events $E$ and $F$ happen. 
    \item $E\mid F$: $E$ happens given that $F$ has happened.
    \item $F\mid E$: $F$ happens given that $E$ has happened.
\end{enumerate}
There is an interesting relationship between these events given by 

The term $P(F\mid E)$ is called the conditional probability of the event "F given E", in fact we use the multiplication principle to formally define conditional proability


The multiplication principle informs us that the probability of the simultaneous occurrence of event $E$ and event $F$, that is $P(E\cap F)$, depends not only on information about the individual occurrence (with no additional information about the other event) of $E$ (that is $P(E)$) and $F$ (that is $P(F)$), \textbf{but} also the interaction/influence of how the event $E$ affects the occurrence of event $F$ (given by the conditional probability $P(F \mid E)$) or how the event $F$ affects the occurrence of event $E$ (given by the conditional probability $P(E \mid F)$).
\\

This naturally leads to the notion of independence, which gives the probabilistic description of what it means for the lack of interaction between two events. 
\\



Observe that, $P(E \mid F) = E(E)$, tells us that the fact that $F$ happened did not affect the probability of $E$ happening. 
\\


An experiment that can be broken down into a sequence of steps can be represented as tree, where nodes at level $i$ are the outcomes of the $(i-1)$th step, and each node at level $i$ will emanate as many nodes as there are possible outcomes for the $i$th step at that node. Every edge emanating out of a node at level $i$ denotes the conditional event of the outcome at the tail of edge happens given that we are node at the head of the edge. 
\\
A unidirectional path from the top of the tree to one of its leaves will describe one outcome for the experiment. The sample space consists of all possible unidirectional paths from the top of the tree to it's leaves. 
\\
The multiplication principle can be used to assign probabilities to all simple events. 


\section{Bayes' Theorem}
We are now ready to state the general theorems  

The last equation in the theorem is just the application of the multiplication principle. 



\begin{proof}
Using the fact that $\{A_1, A_2, \dots, A_k\}$ is a partition and the law of total probability we have:
    \begin{eqnarray*}
    P(A_i \mid B) &=& \frac{P(A_i \cap B)}{P(B)}\\
    &=& \frac{P(A_i \cap B) }{\sum_{j=1}^k P(B\cap A_j)}\\
    &=&\frac{P(A_i)P(B\mid A_i)}{\sum_{j=1}^k P(A_j) P(B \mid A_j)}
\end{eqnarray*}
\end{proof}












\section[Probability and Counting]{Probability and Counting}
Suppose we are in the setting where the sample space $\mathcal{S}$ satisfies the following two conditions:
\begin{enumerate}
\item $\mathcal{S}$ is a finite set. That is, $\mathcal{S}$ is the sample space of an experiment with finitely many outcomes.
\item Every outcome in $\mathcal{S}$ is equally likely. That is, if $\mathcal{S}=\{s1,s_2,\cdots,s_n\}$ then $P(\{s_i\})=\frac{1}{n}$ for all $i=1,2,3,\cdots,n$
\end{enumerate}
Let $A\subset \mathcal{S}$ be an event. Since every outcome in $\mathcal{S}$ is equally likely 
\begin{eqnarray*}
P(A)&=&\text{Probability that A occurs}\\
&=& \sum_{s\in A} P(\{s\})\\
&=& \sum_{s\in A} \frac{1}{n}\\
&=& \frac{n(A)}{n(S)}
\end{eqnarray*}

where $n(A)$ is the number of objects in $A$. 
\\
Therefore, when the sample space is finite and all outcomes in the sample space are equally likely, calculating the probability of an event $A$ is same as counting outcomes in $A$. In this scenario, we only need to be able to count! 
\\

We state the first important theorem of counting. 
 

\begin{proof}
    We prove this theorem using the principle of mathematical induction applied to the number of tasks. We can structure the task $T$ as a tree, with level $i$ in the tree corresponding to the $i$th task $T_i$. 
\end{proof}



\subsection{Choosing from $n$ distinct objects}
We will often want to count the number of ways of selecting $k$ objects from a set of $n$ distinct objects.
\begin{center}
\begin{tabular}{l|l|l|}
\cline{2-3}
                                                                                       & Without Replacement                                                                                    & With Replacement                                                                                   \\ \hline
\multicolumn{1}{|l|}{Ordered}                                                    & \begin{tabular}[c]{@{}l@{}}(1,2,4,5) different from (1,5,2,4)\\ (1,1,2,5) is not possible\end{tabular} & \begin{tabular}[c]{@{}l@{}}(1,2,4,5) different from (1,5,2,4)\\ (1,1,2,5) is possible\end{tabular} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Unordered\end{tabular}} & \begin{tabular}[c]{@{}l@{}}(1,2,3,4) is same as (4,3,2,1)\\ (1,1,2,5) not possible\end{tabular}        & \begin{tabular}[c]{@{}l@{}}(1,2,3,4) is same as (4,3,2,1)\\ (1,1,2,4) is possible\end{tabular}     \\ \hline
\end{tabular}
\end{center}


We use the Fundamental Theorem of Counting to count the number of ways we can choose $k$ objects from $n$ distinct objects as follows\\

\begin{enumerate}
    \item \textbf{Order matters and without replacement}\\
    Let $T$ be the task of choosing $k$ objects from $n$ distinct objects, where order matters and without replacement.  The task $T$ is the same is filling $k$ ordered spaces, and can be broken into a sequence of sub-tasks
    
$$T:T_1\rightarrow T_2\rightarrow T_3 \rightarrow \cdots \rightarrow T_k$$
where $T_i$ fills the $i$th spot in the $k$ ordered spaces. 

Using the fundamental theorem of counting the total number of ways of doing the task $T$ is 
$$n\times (n-1)\times (n-2) \times (n-3) \times \cdots \times (n-k+1)$$

This number is called "$n$ permute $k$" and is denoted by

$$\Perm{n}{k}=\frac{n!}{(n-k)!}$$

\end{enumerate}

(2) \textbf{Unordered Without Replacement}\\
The first step is to calculate the ordered arrangements, which is \begin{align*}
\Perm{n}{k}=\frac{n!}{(n-k)!}
\end{align*}
Note that each ordered arrangement can be rearranged $k!$ times since we select $k$ objects in total. Therefore, we need to get rid repeats by dividing by $k!$, which is \begin{align*}
\frac{\Perm{n}{k}}{k!}=\frac{n!}{(n-k)!k!}
\end{align*}
So we got\begin{align*}
\Comb{n}{k}=\binom nk:=\frac{n!}{(n-k)!k!}
\end{align*}
\hfill\\
\hfill\\
(3) \textbf{With Replacement}\\
The number of ways if choose k objects where order does matter and with replacement in n different things is simply \begin{align*}
&n\times n \times n \times \cdots\times n \\
=&n^k
\end{align*}
\hfill\\
\hfill\\
(4) \textbf{ With replacement and order does not matter}\\
We want number of unordered arrangements of size $k$ from $n$ objects with replacement. This can be reformulate as the number of ways to choose $k$ "walls" from $(n+k-1)$ choices, i.e \begin{align*}
\Comb{n+k-1}{k}=\binom{n+k-1}{k} \text{ ways}
\end{align*} 


The \textbf{number of possible arrangement of size k from n objects}
\textbf{1. Without replacement and order matters} for each of the four possibilities is listed below \begin{center}
\begin{tabular}{l|l|l|}
\cline{2-3}
                                & Without Replacement                                                                                     & With Replacement  \\ \hline
\multicolumn{1}{|l|}{Ordered}   & \begin{tabular}[c]{@{}l@{}}$\Perm{n}{k}=\frac{n!}{n-k}$\\ is also called "n permute k"\end{tabular}     & $n^k$             \\ \hline
\multicolumn{1}{|l|}{Unordered} & \begin{tabular}[c]{@{}l@{}}$\Comb{n}{k}=\frac{n!}{(n-k)!k!}$\\ is also called "n choose k"\end{tabular} & $\Comb{n+k-1}{k}$ \\ \hline
\end{tabular}
\end{center}







