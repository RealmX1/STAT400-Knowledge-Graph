chapter_name,section_name,block_type,block_content
Probability,"Sets, Experiment, and Probability",defn,"[Experiment]
An \textbf{Experiment} is a repeatable task with well defined outcomes."
Probability,"Sets, Experiment, and Probability",defn,"[Sample Space]
The \textbf{Sample Space} for an experiment is the set of all possible outcomes of that experiment, usually denoted by $\mathcal{S}$"
Probability,"Sets, Experiment, and Probability",defn,"[Event]
Any subset $E$ of the sample space $\mathcal{S}$ attached to an experiment will be called an \textbf{Event} associated to the experiment."
Probability,"Sets, Experiment, and Probability",defn,"[Simple Event]
An event is called a  simple event if it has exactly one outcome in it."
Probability,"Sets, Experiment, and Probability",ex,"Experiments involving coin tosses:
    \begin{enumerate}
\item[(a).] Experiment: Toss a coin once. \\
All possible outcomes are contained in the set $\{H,T\}$, with $H$ representing obtaining a head and $T$ representing obtaining a tail. The total number of outcomes is 2. 
\item[(b).] Experiment: Toss the coin two times.\\
All possible outcomes are contained in the set $\{HH, HT, TH, TT\}$. Number of outcomes is 4. 
\item[(c).] Experiment: Toss the coin 10 times.\\
All possible outcomes are contained in the set $\{HHH\cdots H,\ HTHH\cdots H,\ \cdots\}$. Number of outcomes is $2^{10}=1024$. 
\end{enumerate}"
Probability,"Sets, Experiment, and Probability",ex,"Experiments involving die rolls:
\begin{enumerate}
\item[(a).] Roll a six-sided die once. 
Outcomes are in the set $\{1,2,3,4,5,6\}$.
\item[(b).] Roll the die two times. Outcomes are in the set
\begin{align*}
\left\{
\begin{matrix}
(1,1), & (1,2), & (1,3), & \cdots , & (1,6),\\
(2,1), & (2,2), & (2,3), & \cdots , & (2,6),\\
\vdots & \vdots & \vdots & \ddots  & \vdots\\
(6,1), & (6,2), & (6,3), & \cdots , & (6,6)
\end{matrix}
\right\}\ .
\end{align*}
\end{enumerate}"
Probability,"Sets, Experiment, and Probability",ex,"Consider the following two step experiment: In step 1, we toss a coin; In step 2, we roll a 6-sided die. All possible outcomes are contained in the set
\begin{align*}
\left\{
\begin{matrix}
(H,1), &(H,2), &(H,3),& (H,4),& (H,5),& (H,6), \\
(T,1),& (T,2), &(T,3), &(T,4),& (T,5), &(T,6) \\
\end{matrix}
\right\}\ .
\end{align*}"
Probability,"Sets, Experiment, and Probability",ex,"We can perform an experiment where we pick a random student from campus, and ask if they have walked more than 3000 steps today. All possible outcomes of this experiment are contained in the set $\{``\text{Yes}"",\,``\text{No}""\}$."
Probability,"Sets, Experiment, and Probability",ex,"We can perform an experiment where we pick a student from campus and measure their height. All possible outcomes are contained in the set of positive real numbers, that is $(0, \infty)$."
Probability,"Sets, Experiment, and Probability",defn,"[Empty Set]
    An an empty set is a set with nothing in it, usually denoted as $\emptyset$."
Probability,"Sets, Experiment, and Probability",defn,"[Containment/Subset]
    We say $A$ is a \textbf{subset} of $B$ if \textit{$x\in A$ \textbf{implies} $x\in B$}, denoted as $A\subset B$.\\"
Probability,"Sets, Experiment, and Probability",defn,"[Equality]
    We say $A$ \textbf{equals} to $B$ if \textit{$A\subset B$ \textbf{and} $B\subset A$}."
Probability,"Sets, Experiment, and Probability",defn,"[Complement]
    The \textbf{complement} of A is the set $A^c:=\{x:x\notin A\}$
    \begin{center}
\begin{tikzpicture}
    % Definition of circles
    \def\firstcircle{(0,0) circle (1.5cm)}
    \def\secondcircle{(0:2cm) circle (1.5cm)}
    %
    \colorlet{circle edge}{black!50}
    \colorlet{circle area}{gray!20}
    \begin{scope}[local bounding box = orScope]
      \begin{scope}
        \clip \secondcircle;
        \fill[filled] \secondcircle;
    \end{scope}
    \draw[outline] \firstcircle node {$A$};
    \draw[outline] \secondcircle node {$B$};
    \node[anchor=south] at (orScope.north) {$U$};  
    \end{scope}
    \node[fit=(orScope), fill=gray!20] {};
    \node[fit=(orScope), draw] {};
     \begin{scope}
        \clip \secondcircle;
        \fill[filled] \secondcircle;
    \end{scope}
    \begin{scope}
        \clip \firstcircle;
        \fill[white] \firstcircle;
    \end{scope}
    \draw[outline] \firstcircle node {$A$};
    \draw[outline] \secondcircle node {$B$};
    \node[anchor=south] at (orScope.north) {$U$};  
\end{tikzpicture}
\end{center}"
Probability,"Sets, Experiment, and Probability",defn,"[Union]
    The \textbf{union} of A and B is the set $A\cup B:=\{x:x\in A \text{ or }x\in B\}$
    \begin{center}
\begin{tikzpicture}
    % Definition of circles
    \def\firstcircle{(0,0) circle (1.5cm)}
    \def\secondcircle{(0:2cm) circle (1.5cm)}
    %
    \colorlet{circle edge}{black!50}
    \colorlet{circle area}{gray!20}
    %
    \begin{scope}[local bounding box = orScope]
    \draw[filled] \firstcircle node {$A$}
                  \secondcircle node {$B$};
    \node[anchor=south] at (orScope.north){$U$};
    \end{scope}
    \node[fit=(orScope), draw] {};% The frame around the scope
\end{tikzpicture}
\end{center}"
Probability,"Sets, Experiment, and Probability",defn,"[Intersection]
    The \textbf{intersection} of A and B is the set $A\cap B:=\{x:x\in A\text{ and }x\in B\}$
    \begin{center}
\begin{tikzpicture}
    % Definition of circles
    \def\firstcircle{(0,0) circle (1.5cm)}
    \def\secondcircle{(0:2cm) circle (1.5cm)}
    %
    \colorlet{circle edge}{black!50}
    \colorlet{circle area}{gray!20}
    \begin{scope}[local bounding box = orScope]
    \begin{scope}
        \clip \firstcircle;
        \fill[filled] \secondcircle;
    \end{scope}
    \draw[outline] \firstcircle node {$A$};
    \draw[outline] \secondcircle node {$B$};
    \node[anchor=south] at (orScope.north) {$U$};
    \end{scope}
    \node[fit=(orScope), draw] {};
\end{tikzpicture}
\end{center}"
Probability,"Sets, Experiment, and Probability",defn,"[Difference]
   The \textbf{difference} of set $A$ from set $B$ is defined as $A\setminus B:=\{ x: x\in A\; \text{and}\; x\notin B\}$.
   This is the same as $A\setminus B := A\cap B^c$. 
    \begin{center}
\begin{tikzpicture}
    % Definition of circles
    \def\firstcircle{(0,0) circle (1.5cm)}
    \def\secondcircle{(0:2cm) circle (1.5cm)}
    %
    \colorlet{circle edge}{black!50}
    \colorlet{circle area}{gray!20}
    \begin{scope}[local bounding box = orScope]
    \begin{scope}
        \clip \firstcircle;
        \fill[filled] \firstcircle;
    \end{scope}
    \begin{scope}
        \clip \secondcircle;
        \fill[white] \secondcircle;
    \end{scope}
    \draw[outline] \firstcircle node {$A$};
    \draw[outline] \secondcircle node {$B$};
    \node[anchor=south] at (orScope.north) {$U$};
    \end{scope}
    \node[fit=(orScope), draw] {};
\end{tikzpicture}
\end{center}"
Probability,"Sets, Experiment, and Probability",defn,"[Disjoint Sets/Mutually Exclusive]
$A$ and $B$ are said to be \textbf{disjoint} if $A\cap B=\emptyset$."
Probability,"Sets, Experiment, and Probability",defn,"[Pairwise Disjoint Collection]
We say $\{A_i\}_{i\in\mathbb{N}}$ is a collection of pairwise disjoint sets if $A_i\cap A_j=\emptyset$ for all $i\neq j$"
Probability,"Sets, Experiment, and Probability",defn,"[Partition]
A collection $\{A_i\}_{i\in\mathbb{N}}$ is said to be a \textbf{partition} of $\mathcal{S}$ if $\{A_i\}_{i\in\mathbb{N}}$ are pairwise disjoint \textbf{and} $\mathcal{S}=\bigcup_{i=1}^{\infty}A_i$"
Probability,"Sets, Experiment, and Probability",defn,"[Countable Union/Intersection]
Let $\{A_i\}_{i\in\mathbb{N}}$ be a countable collection of subsets of $\mathcal{S}$.
\\
The \textbf{union} of $\{A_i: i \in \mathbb{N}\}$ is defined as \begin{align*}
\bigcup_{i\in \mathbb{N}}A_i &:=\{x : x\in A_i \;\text{ for some }\; i  \in \mathbb{N}\}
\end{align*}
The \textbf{intersection} of $\{A_i: i \in \mathbb{N}\}$  is defined as \begin{align*}
\bigcap_{i\in \mathbb{N}}A_i&=\{x : x\in A_i\; \text{ for all }\; i \in \mathbb{N}\}
\end{align*}"
Probability,"Sets, Experiment, and Probability",defn,"[Sigma Algebra]
Let $\mathcal{S}$ be a sample space of an experiment. A collection, $\mathcal{B}$, of subset of $\mathcal{S}$ is called a \textbf{sigma algebra} is it satisfies the following conditions:
\begin{enumerate}
    \item $\emptyset \in \mathcal{B}$
    \item If $A \in \mathcal{B}$, then $A^c \in \mathcal{B}$
    \item If $\{A_i: i \in \mathbb{N}\}$ is countable collection such that $A_i \in \mathcal{B}$ for all $i$, then $\bigcup_{i\in \mathbb{N}}A_i \in \mathcal{B}$
\end{enumerate}"
Probability,"Sets, Experiment, and Probability",defn,"[Probability Function]
    Consider the pair $(\mathcal{S}, \mathcal{B})$, where $\mathcal{S}$ is the sample space of an experiment and $\mathcal{B}$ is a sigma algebra associated to $\mathcal{S}$. 
    \\
    A probability function for the pair $(\mathcal{S}, \mathcal{B})$, is a function 
    $$P: \mathcal{B} \longrightarrow \mathbb{R}$$
    satisfying the following axioms:
    \begin{enumerate}
    \item (finite measure) $P(\mathcal{S})=1$
    \item (positivity) $P(A)\geq 0$ for all $A \in \mathcal{B}$
    \item (countable additivity) For $A_1,A_2,A_3,\cdots$, the collection of pairwise disjoint subsets of $\mathcal{S}$ in $\mathcal{B}$, we must have
    $$P\left(\bigcup_{i\in \mathbb{N}}A_i\right)=\sum_{i=1}^{\infty}P(A_i)$$ 
    \end{enumerate}"
Probability,"Sets, Experiment, and Probability",thm,"[Properties of a Probability Function]
Suppose $P$ is a probability function.
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\roman*)]
\item $P(\emptyset)=0$
\item If $A \subset B$, then $P(A)\le P(B)$
\item $P(A^c)=1-P(A)$
\item $P(A\setminus B)=P(A)-P(A\cap B)$
\item $P(A\cup B) =P(A)+P(B)-P(A\cap)B$
\end{enumerate}"
Probability,"Sets, Experiment, and Probability",thm,"Suppose the sample space of an experiment is countable, listed as, $\mathcal{S} = \{s_i\}_{i\in \mathbb{N}} = \{s_1, s_2, s_3, \dots, \}$. Let $\mathcal{B}:=\mathcal{P}(\mathcal{S})$ be the powerset of $\mathcal{S}$. 
    \\
    Suppose $\{p_i\}_{i\in \mathbb{N}}$ is a sequence of real numbers satisfying:
    $$ p_i \ge 0 \quad \forall i, \quad \quad \sum_{i=1}^\infty p_i = 1$$
    define $P:\mathcal{B} \rightarrow [0,1]$
    as follows:
    \begin{enumerate}
        \item For any $s_i \in \mathcal{S}$, $$P(\{s_i\}) = p_i$$
        \item For any $E \in \mathcal{B}$
        $$P(E) = \sum_{s\in E} P(\{s\})$$
    \end{enumerate}
    The function $P$ defines a probability function on $\mathcal{B}$."
Probability,"Sets, Experiment, and Probability",ex,"Consider the experiment consisting of tossing a coin two times. The sample space is $S = \{HH, HT, TH, TT\}$. Take the largest possible sigma algebra, that is the power set of $S$, denoted as $\mathcal{P}(S)$, which contains $|\mathcal{P}(S)| = 2^4 = 16$ elements. In this experiment, if the coin is a fair coin, we can assign the following probability function $P: \mathcal{P}(S)\to \R$:
\begin{center}
\begin{tabular}{|c|c|}
\hline \rowcolor{lightgray}
\textbf{event $E$} & $P(E)$\\
\hline
$\emptyset$ & 0 \\
\hline
$\{HH\}$ & 1/4\\
\hline
$\{HT\}$ & 1/4\\
\hline
$\{TT\}$ & 1/4\\
\hline
$\{TH\}$ & 1/4\\
\hline
$\{HH, HT\}$ & 1/4+1/4 = 1/2\\
\hline
\vdots & \vdots \\
\hline
$\{HH, HT, TH\}$ & 1/4+1/4+1/4 = 3/4\\
\hline 
\vdots & \vdots \\
\hline
$\{HH, HT, TH, TT\}$ & 1\\
\hline
\end{tabular}\,.
\end{center}
In the case where the coin is not a fair coin, the probability function can be defined in the following way:
\begin{center}
\begin{tabular}{|c|c|}
\hline \rowcolor{lightgray}
\textbf{event $E$} & $P(E)$\\
\hline
$\emptyset$ & 0 \\
\hline
$\{HH\}$ & 1/3\\
\hline
$\{HT\}$ & 1/3\\
\hline
$\{TT\}$ & 1/3\\
\hline
$\{TH\}$ & 0\\
\hline
$\{HH, TH\}$ & 1/3+0 = 1/3\\
\hline
\vdots & \vdots \\
\hline
$\{HH, HT, TT\}$ & 1/3+1/3+1/3 = 1\\
\hline 
\vdots & \vdots \\
\hline
$\{HH, HT, TH, TT\}$ & 1\\
\hline
\end{tabular}\,.
\end{center}
Similarly, the following probability function is also allowed:
\begin{center}
\begin{tabular}{|c|c|}
\hline \rowcolor{lightgray}
\textbf{event $E$} & $P(E)$\\
\hline
$\emptyset$ & 0 \\
\hline
$\{HH\}$ & 1/8\\
\hline
$\{HT\}$ & 1/8\\
\hline
$\{TT\}$ & 3/8\\
\hline
$\{TH\}$ & 3/8\\
\hline
$\{HH, TH\}$ & 1/8+3/8 = 1/2\\
\hline
\vdots & \vdots \\
\hline
$\{HH, HT, TT\}$ & 1/8+1/8+3/8 = 5/8\\
\hline 
\vdots & \vdots \\
\hline
$\{HH, HT, TH, TT\}$ & 1\\
\hline
\end{tabular}\,.
\end{center}
Notice that the probabilities of events like $\{HH, TH\}$ and $\{HH, HT, TT\}$ are completely determined by the probabilities of the four simple events $\{HH\}$, $\{HT\}$, $\{TH\}$ and $\{TT\}$. While for event $E$, we note that $P(E)$ must be non-negative. We also observe that $P(S) = 1$, and as $S = S\cup \emptyset$, then $P(S) = P(S\cup \emptyset) = P(S) + P(\emptyset) = 1$, from which we deduce that we must have $P(\emptyset) = 0$. The tables shown above are called the distribution tables, and we see that there can be different distribution tables for the same sample space."
Probability,Conditional Probability and Independence,defn,"[Conditional Event]
    Suppose $E$ and $F$ are two events, the conditional event $E\mid F$ - verbalized as ""E given F"" - is the event that ""E happens given that F has already happened"""
Probability,Conditional Probability and Independence,defn,"[Multiplication Principle]
    Suppose $E$ and $F$ are two events then we define:
    $$P(E\cap F) = \begin{cases} P(E)P(F\mid E) \\
    P(F)P(E\mid F)
    \end{cases}$$"
Probability,Conditional Probability and Independence,defn,"[Conditional Probability]
    Suppose $E$ and $F$ are two events then we define:
    $$P(E\mid F) = \frac{P(E\cap F)}{P(F)} $$"
Probability,Conditional Probability and Independence,defn,"[Independent Events]
    We say two events $E$ and $F$ are independent if $$P(E\cap F) = P(E) P(F)$$
    Equivalently, $E$ and $F$ are independent if
    $$ P(E\mid F) = E(E) \quad \text{or}\quad  P(F\mid E) = P(F)$$"
Probability,Conditional Probability and Independence,ex,"We revisit the experiment where we want to assign probabilities to the simple events associated to tossing a coin two times. 
\begin{enumerate}
\item Consider the experiment that we toss a fair coin two times. \\

\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{You\\};
\draw (m-1-1.east) --+ (2.5,0) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (2.5,-2) node[right] (t) {$T_1$};
\draw (h.east) --+ (2.5,0) node[right] (hh) {$H_2$};
\draw (h.east) --+ (2.5,-1) node[right] (ht) {$T_2$};
\draw (t.east) --+ (2.5,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (2.5,-1) node[right] (tt) {$T_2$};
\dr{tt}{TT}{$P(TT)=1/4$}{2.5}{0}
\dr{th}{TH}{$P(TH)=1/4$}{2.5}{0}
\dr{ht}{HT}{$P(HT)=1/4$}{2.5}{0}
\dr{hh}{HH}{$P(HH)=1/4$}{2.5}{0}         

\node[xshift=3.5cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 1};
\node[xshift=7cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 2};
\node[xshift=11cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {$P(E)$};

\draw (m-1-1.east) edge[""1/2""] (h.west);
\draw (m-1-1.east) edge node[below]{1/2} (t.west);
\draw (h.east) edge[""1/2""] (hh.west);
\draw (h.east) edge node[below]{1/2} (ht.west);
\draw (t.east) edge[""1/2""] (th.west);
\draw (t.east) edge node[below]{1/2} (tt.west) ;
\end{tikzpicture}\\

\qquad\qquad
\begin{tabular}{|c|c|c|c|c|}
\hline
 \cellcolor{lightgray} Event $E$ & $HH$ & $HT$ & $TH$ & $TT$\\
\hline
 \cellcolor{lightgray} $P(E)$ & $1/4$ & $1/4$ & $1/4$ &$1/4$\\
\hline 
\end{tabular}\\

\item Now consider first we flip a fair coin, then an unfair coin. \\
The second coin has probability $P(H_2) = 1/4$ and $P(T_2) = 3/4$. 

\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{You\\};
\draw (m-1-1.east) --+ (2.5,0) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (2.5,-2) node[right] (t) {$T_1$};
\draw (h.east) --+ (2.5,0) node[right] (hh) {$H_2$};
\draw (h.east) --+ (2.5,-1) node[right] (ht) {$T_2$};
\draw (t.east) --+ (2.5,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (2.5,-1) node[right] (tt) {$T_2$};
\dr{tt}{TT}{$P(TT)=3/8$}{2.5}{0}
\dr{th}{TH}{$P(TH)=1/8$}{2.5}{0}
\dr{ht}{HT}{$P(HT)=3/8$}{2.5}{0}
\dr{hh}{HH}{$P(HH)=1/8$}{2.5}{0}         

\node[xshift=3.5cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 1};
\node[xshift=7cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 2};
\node[xshift=11cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {$P(E)$};

\draw (m-1-1.east) edge[""1/2""] (h.west);
\draw (m-1-1.east) edge node[below]{1/2} (t.west);
\draw (h.east) edge[""1/4""] (hh.west);
\draw (h.east) edge node[below]{3/4} (ht.west);
\draw (t.east) edge[""1/4""] (th.west);
\draw (t.east) edge node[below]{3/4} (tt.west) ;
\end{tikzpicture}\\

\qquad\qquad
\begin{tabular}{|c|c|c|c|c|}
\hline
 \cellcolor{lightgray} Event $E$ & $HH$ & $HT$ & $TH$ & $TT$\\
\hline
 \cellcolor{lightgray} $P(E)$ & $1/8$ & $3/8$ & $1/8$ &$3/8$\\
\hline 
\end{tabular}\\



\item Now consider first we flip an unfair coin, then a fair coin. \\
The first coin has probability $P(H_1) = 1/4$ and $P(T_1) = 3/4$. 

\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{You\\};
\draw (m-1-1.east) --+ (2.5,0) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (2.5,-2) node[right] (t) {$T_1$};
\draw (h.east) --+ (2.5,0) node[right] (hh) {$H_2$};
\draw (h.east) --+ (2.5,-1) node[right] (ht) {$T_2$};
\draw (t.east) --+ (2.5,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (2.5,-1) node[right] (tt) {$T_2$};
\dr{tt}{TT}{$P(TT)=3/8$}{2.5}{0}
\dr{th}{TH}{$P(TH)=3/8$}{2.5}{0}
\dr{ht}{HT}{$P(HT)=1/8$}{2.5}{0}
\dr{hh}{HH}{$P(HH)=1/8$}{2.5}{0}         

\node[xshift=3.5cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 1};
\node[xshift=7cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 2};
\node[xshift=11cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {$P(E)$};

\draw (m-1-1.east) edge[""1/4""] (h.west);
\draw (m-1-1.east) edge node[below]{3/4} (t.west);
\draw (h.east) edge[""1/2""] (hh.west);
\draw (h.east) edge node[below]{1/2} (ht.west);
\draw (t.east) edge[""1/2""] (th.west);
\draw (t.east) edge node[below]{1/2} (tt.west) ;
\end{tikzpicture}\\

\qquad\qquad
\begin{tabular}{|c|c|c|c|c|}
\hline
 \cellcolor{lightgray} Event $E$ & $HH$ & $HT$ & $TH$ & $TT$\\
\hline
 \cellcolor{lightgray} $P(E)$ & $1/8$ & $1/8$ & $3/8$ &$3/8$\\
\hline 
\end{tabular}\\

\item One might ask if it is possible to construct the following distribution table:\\

\qquad\qquad
\begin{tabular}{|c|c|c|c|c|}
\hline
 \cellcolor{lightgray} Event $E$ & $HH$ & $HT$ & $TH$ & $TT$\\
\hline
 \cellcolor{lightgray} $P(E)$ & $1/8$ & $1/8$ & $4/8$ &$2/8$\\
\hline 
\end{tabular}\\

To construct such a distribution table, we consider the following experiment: In step 1, we toss a coin with $P(H_1) = 1/4$; In step 2, if we got $H_1$ in step 1, then we toss a coin with $P(H_2) = 1/2$, if we got $T_1$ in step 1, then we toss a coin with $P(H_2) = 4/6$.


\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{You\\};
\draw (m-1-1.east) --+ (2.5,0) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (2.5,-2) node[right] (t) {$T_1$};
\draw (h.east) --+ (2.5,0) node[right] (hh) {$H_2$};
\draw (h.east) --+ (2.5,-1) node[right] (ht) {$T_2$};
\draw (t.east) --+ (2.5,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (2.5,-1) node[right] (tt) {$T_2$};
\dr{hh}{HH}{$P(HH)=1/8$}{2.5}{0}         
\dr{ht}{HT}{$P(HT)=1/8$}{2.5}{0}
\dr{th}{TH}{$P(TH)=4/8$}{2.5}{0}
\dr{tt}{TT}{$P(TT)=2/8$}{2.5}{0}

\node[xshift=3.5cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 1};
\node[xshift=7cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 2};
\node[xshift=11cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {$P(E)$};

\draw (m-1-1.east) edge[""1/4""] (h.west);
\draw (m-1-1.east) edge node[below]{3/4} (t.west);
\draw (h.east) edge[""1/2""] (hh.west);
\draw (h.east) edge node[below]{1/2} (ht.west);
\draw (t.east) edge[""4/6""] (th.west);
\draw (t.east) edge node[below]{2/6} (tt.west) ;
\end{tikzpicture}\\

We see that this experiment gives the desired distribution table.

\item Now suppose we toss a two-headed coin two times, that is, $P(H_1) = P(H_2) = 1$.

\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{You\\};
\draw (m-1-1.east) --+ (2.5,0) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (2.5,-2) node[right] (t) {$T_1$};
\draw (h.east) --+ (2.5,0) node[right] (hh) {$H_2$};
\draw (h.east) --+ (2.5,-1) node[right] (ht) {$T_2$};
\draw (t.east) --+ (2.5,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (2.5,-1) node[right] (tt) {$T_2$};
\dr{tt}{TT}{$P(TT)=0$}{2.5}{0}
\dr{th}{TH}{$P(TH)=0$}{2.5}{0}
\dr{ht}{HT}{$P(HT)=0$}{2.5}{0}
\dr{hh}{HH}{$P(HH)=1$}{2.5}{0}         

\node[xshift=3.5cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 1};
\node[xshift=7cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {\bf step 2};
\node[xshift=11cm,text width=2.5cm,above=.5cm] at (m-1-1.north east) {$P(E)$};

\draw (m-1-1.east) edge[""1""] (h.west);
\draw (m-1-1.east) edge node[below]{0} (t.west);
\draw (h.east) edge[""1""] (hh.west);
\draw (h.east) edge node[below]{0} (ht.west);
\draw (t.east) edge[""1""] (th.west);
\draw (t.east) edge node[below]{0} (tt.west) ;
\end{tikzpicture}\\

\qquad\qquad
\begin{tabular}{|c|c|c|c|c|}
\hline
 \cellcolor{lightgray} Event $E$ & $HH$ & $HT$ & $TH$ & $TT$\\
\hline
 \cellcolor{lightgray} $P(E)$ & $1$ & $0$ & $0$ & $0$\\
\hline 
\end{tabular}\\
\end{enumerate}
\hfill\break
\hfill\break

Observe that in the coin toss examples above, we can consider the the following two tree diagram\\
\begin{center}
\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{Experimentor\\};
\draw (m-1-1.east) --+ (1,0.7) node[right] (h) {$H_1$};
\draw (m-1-1.east) --+ (1,-0.7) node[right] (t) {$T_1$};
\draw (h.east) --+ (1,1) node[right] (hh) {$H_2$};
\draw (h.east) --+ (1,0) node[right] (ht) {$T_2$};
\draw (t.east) --+ (1.05,0) node[right] (th) {$H_2$};
\draw (t.east) --+ (1.05,-1) node[right] (tt) {$T_2$};
\dr{tt}{TT}{$T_1T_2$}{1}{0}
\dr{th}{TH}{$T_1H_2$}{1}{0}
\dr{ht}{HT}{$H_1T_2$}{1}{0}
\dr{hh}{HH}{$H_1H_2$}{1}{0}
\dr{HH}{ohh}{$H_1$}{1}{0}
\dr{TH}{oth}{$T_1$}{1.15}{1.4}
\dr{HT}{oht}{$H_1$}{1.15}{-1.4}         
\dr{TT}{ott}{$T_1$}{1.4}{0}
\dr{oth}{oh}{}{1.1}{0}         
\dr{ohh}{oh}{$H_2$}{1}{-1}
\dr{ott}{ot}{}{1}{1}
\dr{oht}{ot}{$T_2$}{1}{0}         
\dr{ot}{obs}{Observer}{1.1}{0.7}
\dr{oh}{obs}{}{1}{-0.7}

\node[xshift=5.3cm,text width=2.5cm,above=1.9cm] at (m-1-1.north east) {sample space};

\draw (m-1-1.east) edge node[above, rotate=45]{\tiny $P(H_1)$ \normalsize} (h.west);
\draw (m-1-1.east) edge node[below, rotate=-39]{\tiny $P(T_1)$ \normalsize} (t.west);
\draw (h.east) edge node[above, rotate=50]{\tiny $P(H_2|H_1)$ \normalsize} (hh.west);
\draw (h.east) edge node[below, rotate=0]{\tiny $P(T_2|H_1)$ \normalsize} (ht.west);
\draw (t.east) edge node[above, rotate=0]{\tiny $P(H_2|T_1)$ \normalsize} (th.west);
\draw (t.east) edge node[below, rotate=-45]{\tiny $P(T_2|T_1)$} (tt.west) ;
\draw (ohh.east) edge node[above, rotate=-45]{\tiny $P(H_1|H_2)$} (oh.west) ;
\draw (oth.east) edge node[below, rotate=0]{\tiny $P(T_1|H_2)$} (oh.west) ;
\draw (oht.east) edge node[above, rotate=0]{\tiny $P(H_1|T_2)$} (ot.west) ;
\draw (ott.east) edge node[below, rotate=45]{\tiny $P(T_1|T_2)$} (ot.west) ;
\draw (ot.east) edge node[below, rotate=45]{\tiny $P(T_2)$} (obs.west) ;
\draw (oh.east) edge node[above, rotate=-39]{\tiny $P(H_2)$} (obs.west) ;

\draw[-latex] (0,-2.5) -- ++ (5cm,0) node[midway,below,align=center]{Experiment arrow of time};
\draw[-latex] (15,-2.5) -- ++ (-5cm,0) node[midway,below,align=center]{Observer arrow of time};
\end{tikzpicture}
\end{center}
The connecting piece between these two trees is the law of total probability and Bayes' theorem."
Probability,Bayes' Theorem,thm,"[Law of Total Probability]
Suppose $\{A_1, A_2, \dots, A_k\}$ is a partition for the sample space and $B$ is any event, then
\begin{eqnarray*}
    P(B) &=& \sum_{j=1}^k P(B\cap A_j)\\
    &=&\sum_{j=1}^k P(A_j) P(B \mid A_j)
\end{eqnarray*}"
Probability,Bayes' Theorem,thm,"[Bayes' Theorem]
    Suppose $\{A_1, A_2, \dots, A_k\}$ is a partition for the sample space and $B$ is any event, then
\begin{eqnarray*}
    P(A_i \mid B) = \frac{P(A_i)P(B\mid A_i)}{\sum_{j=1}^k P(A_j) P(B \mid A_j)}
\end{eqnarray*}"
Probability,Bayes' Theorem,ex,"%% Choosing a coin and tossing it two times
A bag contains two coins, one is a fair coin denoted as $F$, and the other is a two headed coin denoted as $U$. \\
We randomly choose a coin from the bag and toss it once, this can be described by the following tree diagram.\\
\begin{center}
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {}
    child {
        node[bag] {U}        
            child {
                node[end, label=right:
                    {\text{H}}] {}
                edge from parent
                node[above]  {$P(H\mid U)=1$}
                node[below] {\begin{scriptsize}\text{Note }$P(T\mid U)=0$ \end{scriptsize} }
            }
            edge from parent 
            node[above] {$P(U)=\frac{1}{2}$}
    }
    child {
        node[bag] {F}        
        child {
                node[end, label=right:
                    {\text{T}}] {}
                edge from parent
                node[above] {$P(T\mid F)=\frac{1}{2}$}
            }
            child {
                node[end, label=right:
                    {\text{H}}] {}
                edge from parent
                nnode[above] {$P(T\mid F)=\frac{1}{2}$}
            }
        edge from parent         
            node[above] {$P(F)=\frac{1}{2}$}
    };
\end{tikzpicture}
\end{center}
The sample space $\mathcal{S}=\{FH,FT,UH\}$, which is $F\cap H$, $F\cap T,U\cap H$ respectively.\\
\hfill\\
We can calculate the probability that the tossed coin lands heads as follows \begin{align*}
P(H)&=P(F\cap H)+P(U\cap H)\\
&=P(H\mid F)\cdot P(F)+P(H\mid U)\cdot P(U)\\
&=\frac{1}{2}\cdot \frac{1}{2}+1\cdot \frac{1}{2}=\frac{1}{4}+\frac{1}{2}=\frac{3}{4}
\end{align*}
Observing that $P(H\mid F)=\frac{1}{2}\neq \frac{3}{4}=P(H)$, helps us conclude that observing a $H$ on the coin toss and choosing the fair coin $F$ are two events that are not \textbf{not} independent of each other. 
\\

We can also calculate the conditional probability
\begin{align*}
P(F|H)&=\frac{P(F\cap H)}{P(H)}\\
&=\frac{P(H\mid F)\cdot P(F)}{P(H)}\\
&=\frac{\frac{1}{2}\cdot \frac{1}{2}}{\frac{3}{4}}=\frac{1}{4}\cdot \frac{4}{3}=\frac{1}{3}
\end{align*}
Therefore, $P(F)=\frac{1}{2}>\frac{1}{3}=P(F|H)$. This makes intuitive sense, as $P(F) = P(\text{tossed coin is a fair coin})$ will be lowered after we observe the $H$ outcome."
Probability,Bayes' Theorem,ex,"Suppose we now randomly choose a coin from the bag containing a fair coin and a two headed coin, and toss it two times independently. 
This experiment can be visualized in the following tree diagram:
\begin{center}
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {}
    child {
        node[bag] {U}        
            child {
                node[bag]{H}
                child{
                 node[end, label=right:
                    {\text{H}}] {}
                edge from parent
                node[above]  {\begin{scriptsize}$P(H\mid H\cap U)=1$\end{scriptsize}}
                }
                edge from parent
                node[above]  {$P(H\mid U)=1$}
                node[below] {\begin{scriptsize}\text{Note }$P(T\mid U)=0$ \end{scriptsize} }
            }
            edge from parent 
            node[above] {\begin{small}$P(U)=\frac{1}{2}$\end{small}}
    }
    child {
        node[bag] {F}        
        child {
                node[bag] {T}
                child{
                 node[end, label=right:
                    {\text{T}}] {}
                edge from parent
                node[above]  {\begin{scriptsize}$P(T\mid T\cap F)=\frac{1}{2}$\end{scriptsize}}
                }
                child{
                 node[end, label=right:
                    {\text{H}}] {}
                edge from parent
                node[above]  {\begin{scriptsize}$P(H\mid T\cap F)=\frac{1}{2}$\end{scriptsize}}
                }
                edge from parent
                node[above] {\begin{small}$P(T\mid F)=\frac{1}{2}$\end{small}}
            }
            child {
                node[bag] {H}
                child{
                 node[end, label=right:
                    {\text{H}}] {}
                edge from parent
                node[above]  {\begin{scriptsize}$P(H\mid H\cap F)=\frac{1}{2}$\end{scriptsize}}
                }
                child{
                 node[end, label=right:
                    {\text{T}}] {}
                edge from parent
                node[above]  {\begin{scriptsize}$P(T\mid H\cap F)=\frac{1}{2}$\end{scriptsize}}
                }
                edge from parent
                node[above] {\begin{small}$P(T\mid F)=\frac{1}{2}$\end{small}}
            }
        edge from parent         
            node[above] {$P(F)=\frac{1}{2}$}
    };
\end{tikzpicture}
\end{center}
The sample space is $\mathcal{S}=\{FHT,FHH,FTH,FTT,UHH\}$. 

We can calculate the probabilities of each of the simple events by taking products along the path to the given outcome, for example:
\begin{eqnarray*}
    P(HHF)&=& P(H\cap (H\cap F))\\
        &=&P(H\mid H\cap F)\cdot P(H\cap F)\\
        &=&P(H\mid H\cap F)\cdot P(H\mid F)\cdot P(F)\\
        &=&\text{Product along path to $HHF$}
\end{eqnarray*}

Suppose we want to calculate $P(HH) = P(\text{the tossed coin lands heads both times})$, we do it as follows:
\begin{eqnarray*}
P(HH)&=&P(FHH)+P(UHH)\\
&=&P(H\mid HF)\cdot P(HF)+P(H\mid HU)\cdot P(HU)\\
&=&\frac{1}{2}\cdot \frac{1}{2}\cdot \frac{1}{2}+\frac{1}{2}\cdot 1\cdot 1\\
&=&\frac{1}{8}+\frac{1}{2}=\frac{5}{8}
\end{eqnarray*}

We can also calculate the conditional probability 
\begin{eqnarray*}
    P(HH\mid F)&=& P(\text{observed two consecutive heads given that a fair coin was tossed}\\
    &=&\frac{P(HHF)}{P(F)}\\
    &=&\frac{\frac{1}{2}\cdot \frac{1}{2}\cdot \frac{1}{2}}{\frac{1}{2}}\\
    &=&\frac{1}{4}
\end{eqnarray*}

Therefore, $P(HH)=\frac{5}{8}>\frac{1}{4}=P(HH\mid F)$, the information that we tossed a fair coin affected the probability of getting two consecutive heads - this information results in a lower probability. 

We could also consider the probability  
\begin{eqnarray*}
P(F\mid HH) &=& P(\text{we tossed a fair coin given that $HH$ was observed})\\
&=& \frac{P(FHH)}{P(HH)}=\frac{P(HH\mid F)\cdot P(F)}{P(HH)}\\
&=&\frac{\frac{1}{4}\cdot \frac{1}{2}}{\frac{5}{8}}=\frac{1}{8}\cdot \frac{8}{5}\\
&=&\frac{1}{5}
\end{eqnarray*}

Therefore, $P(F\mid HH)=\frac{1}{5}<\frac{1}{3}=P(F\mid H)<\frac{1}{2}=P(F)$, which aligns with the intuition that as more heads are observed, our confidence that a fair coin was tossed must decrease."
Probability,Bayes' Theorem,ex,"[Disease Testing]
Suppose we consider a disease in a certain population, and 
$$ D:= \{\text{randomly chosen individual has the disease}\}$$ 
so that 
$$D^c:= \{\text{randomly chosen individual does  not have the disease}\}$$
Observe that $$D\cup D^c := \{ \text{entire population}\} = \mathcal{S}$$

Additionally, we are given a test, which when administered will determine if the person has the disease or not. Suppose 
$$ \text{Pos} := \{\text{test result is positive, i.e. test claims you have the disease}\}$$

$$ \text{Neg} := \{\text{test result is negative, i.e. test claims you do not have the disease}\}$$
\\
If everyone in the population was administered the test, we can observe that $$\text{Pos}\cup \text{Neg} := \{ \text{entire population}\} = \mathcal{S}$$

We now consider the following probabilities:
\begin{eqnarray*}
    P(D)&:=& P(\text{a randomly chosen individual from the population has the disease}) \\
    &:=& p \in (0,1) 
\end{eqnarray*}
this is sometimes called the prevalence of the disease. Additionally, we are given 
\begin{eqnarray*}
    P(\text{test correctly identifies the disease})&:=& q \in (0,1) 
\end{eqnarray*}

This is sometimes called the accuracy of the test. Using this we can calculate the following conditional probabilities:
\begin{eqnarray*}
    P(\text{Pos}\mid D)&=& P(\text{Neg}\mid D^c) = q \\
    P(\text{Pos} \mid D^c) &=& P(\text{Neg}\mid D) = 1-q
\end{eqnarray*}

Depending on the information that is available, we can consider two differently ordered experiments. 
\\

\begin{eqnarray*}
    \text{Step 1} &:=& \text{Check if you have the disease} \\
    \text{Step 2} &:=& \text{What is the test result?}
\end{eqnarray*} 

The tree that we get in this sequence is called the ""Experimenter's Tree"", as the experimenter testing the accuracy of the test needs to know the disease-status of the individual on whom the test is administered. This tree can be visualized as follows:

\qquad\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{Experimenter\\};
\draw (m-1-1.east) --+ (1,0.7) node[right] (h) {$D$};
\draw (m-1-1.east) --+ (1,-0.7) node[right] (t) {$D^c$};
\draw (h.east) --+ (1.1,1) node[right] (hh) {$ \text{Pos} $};
\draw (h.east) --+ (1.1,0) node[right] (ht) {$ \text{Neg} $};
\draw (t.east) --+ (1,0) node[right] (th) {$ \text{Pos} $};
\draw (t.east) --+ (1,-1) node[right] (tt) {$ \text{Neg} $};
\dr{tt}{TT}{$(D^c,\, \text{Neg} )$}{1}{0}
\dr{th}{TH}{$(D^c,\, \text{Pos} )$}{1}{0}
\dr{ht}{HT}{$(D,\, \text{Neg} )$}{1}{0}
\dr{hh}{HH}{$(D,\, \text{Pos} )$}{1}{0}

\node[xshift=5.3cm,text width=2.5cm,above=1.9cm] at (m-1-1.north east) {sample space};

\draw (m-1-1.east) edge node[above, rotate=45]{\tiny $p$ \normalsize} (h.west);
\draw (m-1-1.east) edge node[below, rotate=-39]{\tiny $1-p$ \normalsize} (t.west);
\draw (h.east) edge node[above, rotate=50]{\tiny $q$ \normalsize} (hh.west);
\draw (h.east) edge node[below, rotate=0]{\tiny $1-q$ \normalsize} (ht.west);
\draw (t.east) edge node[above, rotate=0]{\tiny $1-q$ \normalsize} (th.west);
\draw (t.east) edge node[below, rotate=-45]{\tiny $q$} (tt.west) ;
\end{tikzpicture}


From the user's (of the test) point of view, we have the following steps
\begin{eqnarray*}
    \text{Step 1} &:=& \text{administer the test} \\
    \text{Step 2} &:=& \text{Do I have the disease?}
\end{eqnarray*} 
We can see that the user's order of operation is exactly reverse of the experimenter. 
\\
The user of the test is aware of the test result, and based on that information wants to know their disease status. 
\\

We can put all of this information in the following two tree diagrams. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TREE DIAGRAMS %%%%%%%%%%%%%%%%%%%%%%
\begin{center}
\begin{tikzpicture}[>=triangle 60,every node/.style={anchor=west}]
\matrix (m) [matrix of nodes, row sep=0em, column sep=3em]
{Experimenter\\};
\draw (m-1-1.east) --+ (1,0.7) node[right] (h) {$D$};
\draw (m-1-1.east) --+ (1,-0.7) node[right] (t) {$D^c$};
\draw (h.east) --+ (1.1,1) node[right] (hh) {$ \text{Pos} $};
\draw (h.east) --+ (1.1,0) node[right] (ht) {$ \text{Neg} $};
\draw (t.east) --+ (1,0) node[right] (th) {$ \text{Pos} $};
\draw (t.east) --+ (1,-1) node[right] (tt) {$ \text{Neg} $};
\dr{tt}{TT}{$(D^c,\, \text{Neg} )$}{1}{0}
\dr{th}{TH}{$(D^c,\, \text{Pos} )$}{1}{0}
\dr{ht}{HT}{$(D,\, \text{Neg} )$}{1}{0}
\dr{hh}{HH}{$(D,\, \text{Pos} )$}{1}{0}
\dr{HH}{ohh}{$D$}{1}{0}
\dr{TH}{oth}{$D^c$}{1}{1.4}
\dr{HT}{oht}{$D$}{1}{-1.4}         
\dr{TT}{ott}{$D^c$}{1}{0}
\dr{oth}{oh}{}{1}{0}         
\dr{ohh}{oh}{$ \text{Pos} $}{1.5}{-1}
\dr{ott}{ot}{}{1}{1}
\dr{oht}{ot}{$ \text{Neg} $}{1.35}{0}         
\dr{ot}{obs}{User}{1}{0.7}
\dr{oh}{obs}{}{1}{-0.7}

\node[xshift=5.3cm,text width=2.5cm,above=1.9cm] at (m-1-1.north east) {sample space};

\draw (m-1-1.east) edge node[above, rotate=45]{\tiny $p$ \normalsize} (h.west);
\draw (m-1-1.east) edge node[below, rotate=-39]{\tiny $1-p$ \normalsize} (t.west);
\draw (h.east) edge node[above, rotate=50]{\tiny $q$ \normalsize} (hh.west);
\draw (h.east) edge node[below, rotate=0]{\tiny $1-q$ \normalsize} (ht.west);
\draw (t.east) edge node[above, rotate=0]{\tiny $1-q$ \normalsize} (th.west);
\draw (t.east) edge node[below, rotate=-45]{\tiny $q$} (tt.west) ;
\draw (ohh.east) edge node[above, rotate=-35]{\tiny $P(D| \text{Pos} )$} (oh.west) ;
\draw (oth.east) edge node[below, rotate=0]{\tiny $P(D^c| \text{Pos} )$} (oh.west) ;
\draw (oht.east) edge node[above, rotate=0]{\tiny $P(D| \text{Neg} )$} (ot.west) ;
\draw (ott.east) edge node[below, rotate=45]{\tiny $P(D^c| \text{Neg} )$} (ot.west) ;
\draw (ot.east) edge node[below, rotate=45]{\tiny $P( \text{Neg} )$} (obs.west) ;
\draw (oh.east) edge node[above, rotate=-35]{\tiny $P( \text{Pos} )$} (obs.west) ;

\draw[-latex] (0,-2.5) -- ++ (5cm,0) node[midway,below,align=center]{Experimenter's arrow of time};
\draw[-latex] (15,-2.5) -- ++ (-5cm,0) node[midway,below,align=center]{User's arrow of time};
\end{tikzpicture}
\end{center}

In this setting, the accuracy of the test relates to $P(\text{Pos}|D^c)$ and $P(\text{Neg}|D)$ being small, as they correspond to the probabilities of incorrectly identifying the disease using the test. On the other hand, $P(\text{Pos}|D)$ and $P(\text{Neg}|D^c)$ should be large as they correspond to the probabilities of correctly identifying the disease using the test. 
\\

For the observer, one expects $P(D|\text{Pos})$ and $P(D^c| \text{Neg})$ to be large, and $P(D^c|\text{Pos})$ and $P(D|\text{Neg})$ to be small.
\\

Observe that 
\begin{enumerate}
    \item The trees for the experimenter and the observer result in the same sample space.
    \item Using the Law of Total Probability and the Bayes' theorem, we can calculate all the probabilities in the user's tree if the all the probabilities in the experimenter's tree are known. 
\end{enumerate}

We calculate as follows:

\begin{align*}
P(\text{Pos}) &= P(D, \text{Pos})+P(D^c, \text{Pos}) = p \cdot q + (1-p)\cdot (1-q)\,,
\end{align*}
Therefore, 
\begin{align*}
P(D|\text{Pos}) &= \frac{P(D, \text{Pos})}{P(\text{Pos})} = \frac{pq}{pq+(1-p)(1-q)}\,, 
\end{align*}
\begin{align*}
P(D^c| \text{Pos}) = \frac{P(D^c, \text{Pos})}{P(\text{Pos})} = \frac{(1-p)(1-q)}{pq+(1-p)(1-q)}\,.
\end{align*}
Similarly, we can calculate
\begin{align*}
P(\text{Neg}) = p\cdot(1-q) + q\cdot (1-p)\,, 
\end{align*}
\begin{align*}
P(D^c|\text{Neg}) = \frac{q(1-p)}{q(1-p)+p(1-q)}\,,\qquad
P(D|\text{Neg}) = \frac{p(1-q)}{q(1-p) +p(1-q)}\,.
\end{align*}
This finishes the calculation of all the probabilities in the user's tree. 
\\

As mentioned before, we would like to have 
\begin{align*}
P(D|\text{Pos}) = \frac{pq}{pq+(1-p)(1-q)}\,,\qquad
\text{and}\qquad P(D^c|\text{Neg}) = \frac{(1-p)q}{(1-p)q+p(1-q)}
\end{align*}
to be large.
\\

We can consider two special cases:
\begin{enumerate}
\item Suppose the disease is highly prevalent, for instance, $P(D) = p = 0.9999$. That is, 9999 out of 10000 people on average have the disease. Furthermore, let $P(\text{test correctly identifies the disease})= q = 0.9999$. That is, the test is on average correct 9999 times out of 10000 times. For the test to be useful, we want to have both $P(D|\text{Pos})$ and $P(D^c|\text{Neg})$ to be large. In this case, we compute their numerical values
\begin{align*}
P(D|\text{Pos}) = \frac{pq}{pq+(1-p)(1-q)}  = \frac{0.9999 \cdot 0.9999}{0.9999\cdot 0.9999 + 0.0001 \cdot 0.0001}\approx 1\,.
\end{align*}
However, we see that
\begin{align*}
P(D^c|\text{Neg}) = \frac{(1-p)q}{(1-p)q+p(1-q)} = \frac{0.0001 \cdot 0.9999}{0.0001\cdot 0.9999 + 0.9999\cdot 0.0001} = 0.5\,,
\end{align*}
from which we see that a negative test result will correctly identify the test with probability 0.5, perhaps the user is better off tossing a fair coin (instead of buying the test). The test is not very useful in this sense. 

\item On the other hand, if we have a rare disease, say for instance $P(D) = p = 0.0001$. Suppose that the test accuracy is  $P(\text{test correctly identifies disease}) = 0.9999=q$. For the test to be useful, we again want to have both $P(D|\text{Pos})$ and $P(D^c|\text{Neg})$ to be large, but we again see that 
\begin{align*}
P(D|\text{Pos}) = \frac{pq}{pq+(1-p)(1-q)} = \frac{0.0001\cdot 0.9999}{0.0001 \cdot 0.9999 + 0.9999 \cdot 0.0001} = 0.5\,,
\end{align*}
which is again equivalent to tossing a fair coin toss. 
\end{enumerate}"
Probability,Bayes' Theorem,thm,"[Fundamental Theorem of Counting]
 Suppose there are $k$ tasks: $T_1,T_2,\cdots,T_k$ that can be performed in $n_1,n_2,\cdot,n_k$ ways respectively. Let $T$ be the task of performing $T_1,T_2,\cdot,T_k$ sequentially. Then the total number of ways to perform the task $T$ is 
 $$n_1\times n_2 \times \cdots \times n_k$$"
Probability,Bayes' Theorem,ex,Add examples of counting.
Probability,Bayes' Theorem,ex,"Suppose we want to count the total number of ways one can choose 4 digits from the set of 10 digits $\{0,1,2,3,4,5,6,7,8,9\}$.
\\

The answer to this we need to understand how the digits are selected. Do we place the digit back into the set before the subsequent choice (with/without replacement)? Does the order in which the 4 digits are selected matter?
This is leads to the following 4 possibilities:"
Probability,Bayes' Theorem,ex,"We set $n=10,k=3,$ where $S=\{0,1,2,3,4,5,6,7,8,9\}$.\\
Then we will have $n+k-1=12$ walls \begin{center}
 \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 &  &  &  &  &  &  &  &  &  &  &  \\ \hline
\end{tabular}
\end{center}
Then, for the event $(1,1,2)$, we will have (note that X means the wall that we choose) \begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 1 & X & X & 2 & X & 3 & 4 & 5 & 6 & 7 & 8 & 9  \\ \hline
\end{tabular}
\end{center}
For the event $(0,0,7)$, we will have \begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 X & X & 1 & 2 & 3 & 4 & 5 & 6 & 7 & X&8 & 9  \\ \hline
\end{tabular}
\end{center}
for the event $(5,9,7)$, we will have \begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
 1  & 2  & 3 & 4 & 5&X & 6 & 7 &X& 8 & 9&X  \\ \hline
\end{tabular}
\end{center}"
Probability,Bayes' Theorem,ex,"Consider choosing $3$ digits from the set $\{1,2,3,4,5\}$. 
\begin{center}
\begin{tabular}{|c|c|c|}
\hline \rowcolor{lightgray}
 & order matters & order does not matter\\
\hline
\cellcolor{lightgray} with &(1,2,2) is allowed & (1,2,2) is allowed\\
\cellcolor{lightgray} replacement &  but is different from (2,1,2) &  and is the same as (2,1,2)\\
\hline
\cellcolor{lightgray} without &(1,2,2) is not allowed, & (1,2,2) is not allowed,\\
\cellcolor{lightgray} replacement & (1,2,3) is different from (1,3,2) &  (1,2,3) is the same as (1,3,2)\\
\hline
\end{tabular}
\end{center}

In this case, $n=5$ and $k=3$, and we can compute as follows
\begin{align*}
^n\text{P}_k = 
{}^5\text{P}_3 = \frac{5!}{(5-3)!} = 5\cdot 4 \cdot 3 = 60\,, \qquad\quad
^n\text{C}_k = {}^5\text{C}_3
=\frac{5!}{(5-3)!\, 3!} = \frac{60}{3!} = 10\,,
\end{align*}
\begin{align*}
n^k  = 5^3 = 125\,,
\qquad\quad
^{n+k-1}\text{C}_{n-1}=
{}^{7}\text{C}_{4} = \frac{7!}{4!\,3!} = 35\,.
\end{align*}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline \rowcolor{lightgray}
 & order matters & order does not matter\\
\hline
\cellcolor{lightgray} with replacement & 125 & 35\\
\hline
\cellcolor{lightgray} without replacement & 60 & 10\\
\hline
\end{tabular}
\end{center}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we toss a two sided coin three times. The sample space of this experiment is given by: 
    $$\mathcal{S} := \begin{Bmatrix}
         HHH, & HHT, & HTT, &TTT  \\
            & HTH, & THT,& \\
            &THH, & TTH&
    \end{Bmatrix}
    $$
    We can consider numerical features of an outcome in the sample space, for example:
    \begin{enumerate}
        \item How many heads in the outcome?
        \item How many tails in the outcome?
        \item Are there more heads than tails?
        \item Is the first toss in the outcome a head?
        \item Is the last toss a tails?
        \item How many tails till the first head?
    \end{enumerate}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we randomly sample a student from the campus. The sample space for this experiment is given by:
    $$\mathcal{S} : = \{ \text{All students on campus}\}$$
    We can consider numerical features of an outcome (a student from the campus) in the sample space, for example:
    \begin{enumerate}
        \item What is the height of the chosen student?
        \item What is the average daily commute time for the student?
        \item The number of credits the student is enrolled in?
        \item The number of electronic gadgets the student owns?
        \item Average number of people the student interacts with on a given day?
        \item Is the student an athlete?
    \end{enumerate}"
Random Variables,Random Variables: Examples and Definitions,defn,"[Random Variable]
    A random variable $X$ is a real valued function on the sample space $\mathcal{S}$. That is, 
    \begin{eqnarray*}
        X :& \mathcal{S} &\longrightarrow \mathbb{R}\\
        & \omega &\mapsto X(\omega)
    \end{eqnarray*}
    The range of $X$, that is the set of all possible values that $X$ can take will be denoted as $\mathcal{X}$"
Random Variables,Random Variables: Examples and Definitions,defn,"[Events Associated to a Random Variable]
    Given a random variable $$X: \mathcal{S} \longrightarrow \mathbb{R}$$
    we can use the order relationship of $\mathbb{R}$ to consider the following ""interesting events"" for $X$:
    \begin{eqnarray*}
        \{a \le X \le b\} :=& \text{$X$ takes values in the interval $[a, b]$}\\
        \{a \le X \} :=& \text{$X$ takes values in the interval $[a, \infty)$}\\
        \{ X \le b\} :=& \text{$X$ takes values in the interval $[-\infty, b]$}\\
    \end{eqnarray*}
    We can also allow for strict inequalities, countable unions, complements, and countable intersections. 
    \\
    We will assume that we are able to calculate the probabilities associated to these events, and these will be denoted by $P(a\le X \le b), P(a \le X), P(X \le b)$, etc"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose the sample space $\mathcal{S}$ is all students on campus.

For $ \omega \in \mathcal{S}$, we define:
\[
X(\omega) := \text{height of } \omega \text{ (in inches)}
\]

What are the values of \( X \)?

\[
\mathcal{X} := (0, \infty)
\]

Some ""interesting events"" corresponding to $X$ can be:
\begin{eqnarray*}
    \{ 60 \leq X \leq 65 \} &:=& \{ \omega \in S \mid X(\omega) \in [60, 65] \}\\
&=& \{\text{set of all students whose height is between 60 and 65 inches.}\}
\end{eqnarray*}

\begin{eqnarray*}
    \{X \ge 72\} &:=& \{ \omega \in S \mid X(\omega) \in [72, \infty) \}\\
    &=&\{\text{all students whose height is greater than or equal to 72 inches}\} 
\end{eqnarray*}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we toss a coin three times, and we want to know if the first toss was a head?
    \begin{equation*}
    X(\omega) =
    \begin{cases}
    0 & \text{if first toss in } \omega \text{ is not H} \\
    1 & \text{otherwise}
    \end{cases}
    \end{equation*}


\begin{center}
\begin{tikzpicture}[scale=1.2]
\tikzset{every node/.style={font=\footnotesize}}

% Nodes for outcomes
\node at (0, 4) (HHH) {HHH};
\node at (0, 3.5) (HHT) {HHT};
\node at (0, 3) (HTH) {HTH};
\node at (0, 2.5) (HTT) {HTT};
\node at (0, 2) (THH) {THH};
\node at (0, 1.5) (THT) {THT};
\node at (0, 1) (TTH) {TTH};
\node at (0, 0.5) (TTT) {TTT};

% Nodes for outputs
\node at (4, 3.25) (1) {1};
\node at (4, 1.75) (0) {0};



% Lines for 0
\draw[green] (HHH) -- (1);
\draw[green] (HHT) -- (1);
\draw[green] (HTH) -- (1);
\draw[green] (HTT) -- (1);

% Lines for 1
\draw[orange] (THH) -- (0);
\draw[orange] (THT) -- (0);
\draw[orange] (TTH) -- (0);
\draw[orange] (TTT) -- (0);
\end{tikzpicture}
\end{center}

Another way of looking at it:
\begin{equation*}
\begin{aligned}
    \{ X = 0 \} := X^{-1}(0) & = \{ \text{all sample space outcomes on which } X \text{ takes value 0} \} \\
    & = \{ \text{THH, THT, TTH, TTT} \}
\end{aligned}
\end{equation*}
Similarly,
\begin{equation*}
\begin{aligned}
    & \{ X = 1 \} := X^{-1}(1) = \{ \text{HHH, HTH, HHT, HTT} \}
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
    S := \{ & \{ \text{HHH, HTH, HHT, HTT} \} \mapsto \{1\}, \{ \text{THH, THT, TTH, TTT} \} \mapsto \{ 0 \} \}
\end{aligned}
\end{equation*}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we tossed a coin three times, and we were interested in counting the number of heads in three tosses. We can work through this as follows:

\begin{equation*}
X(\omega) := \# \text{ of heads in three tosses.}
\end{equation*}

\begin{center}
\begin{tikzpicture}[scale=1.2]
\tikzset{every node/.style={font=\footnotesize}}

% Nodes for outcomes
\node at (0, 4) (HHH) {HHH};
\node at (0, 3.5) (HHT) {HHT};
\node at (0, 3) (HTH) {HTH};
\node at (0, 2.5) (HTT) {HTT};
\node at (0, 2) (THH) {THH};
\node at (0, 1.5) (THT) {THT};
\node at (0, 1) (TTH) {TTH};
\node at (0, 0.5) (TTT) {TTT};

% Nodes for outputs
\node at (4, 4) (3) {3};
\node at (4, 3) (2) {2};
\node at (4, 2) (1) {1};
\node at (4, 1) (0) {0};




% Lines for 3
\draw[blue] (HHH) -- (3);

% Lines for 2
\draw[cyan] (HHT) -- (2);
\draw[cyan] (HTH) -- (2);
\draw[cyan] (THH) -- (2);

% Lines for 1
\draw[green] (THT) -- (1);
\draw[green] (HTT) -- (1);
\draw[green] (TTH) -- (1);

% Lines for 0
\draw[orange] (TTT) -- (0);
\end{tikzpicture}
\end{center}

\begin{equation*}
\begin{aligned}
    & \{ X = 0 \} = \{ \text{TTT} \} \\
    & \{ X = 1 \} = \{ \text{HTT, THT, TTH} \} \\
    & \{ X = 2 \} = \{ \text{HHT, HTH, THH} \} \\
    & \{ X = 3 \} = \{ \text{HHH} \}
\end{aligned}
\end{equation*}

\begin{equation*}
\mathcal{S} = \left\{
\begin{aligned}
    & \{ \text{HHH} \} \quad && \mapsto 3 \\[4pt]
    & \{ \text{HHT, HTH, THH} \} \quad && \mapsto 2 \\[4pt]
    & \{ \text{HTT, THT, TTH} \} \quad && \mapsto 1 \\[4pt]
    & \{ \text{TTT} \} \quad && \mapsto 0 
\end{aligned} 
\right\}
\end{equation*}

\begin{center}
\begin{tikzpicture}[scale=1.2]
\tikzset{every node/.style={font=\footnotesize}}

% Ellipses and elements inside
\draw (0,0) ellipse (0.5cm and 0.5cm);
\draw (1.5,0) ellipse (0.5cm and 1.5cm);
\draw (3,0) ellipse (0.5cm and 1.5cm);
\draw (4.5,0) ellipse (0.5cm and 0.5cm);

\node at (0, 0) {HHH};

\node at (1.5, -0.5) {HHT};
\node at (1.5, 0) {HTH};
\node at (1.5, 0.5) {THH};

\node at (3, -0.5) {HTT};
\node at (3, 0) {THT};
\node at (3, 0.5) {TTH};

\node at (4.5, 0) {TTT};

%\node at (-1, 0) {$\mathcal{S} = \left\{ \right.$}; % Big left curly brace
%\node at (5.5, 0) {$\left. \right\}$}; % Big right curly brace

\node at (0, -2.5) {\{3\}};
\node at (1.5, -2.5) {\{2\}};
\node at (3, -2.5) {\{1\}};
\node at (4.5, -2.5) {\{0\}};

% Arrows
\draw[->,orange] (0, -1.5) -- (0, -2);
\draw[->,orange] (1.5, -1.5) -- (1.5, -2);
\draw[->,orange] (3, -1.5) -- (3, -2);
\draw[->,orange] (4.5, -1.5) -- (4.5, -2);

\end{tikzpicture}
\end{center}"
Random Variables,Random Variables: Examples and Definitions,defn,"[Cumulative Distribution Function]
    The cummulative distribution function associated to a random variable $$X: \mathcal{S} \longrightarrow \mathbb{R}$$
    is defined as 
    \begin{eqnarray*}
        F_X :& \mathbb{R} &\longrightarrow \mathbb{R}\\
        & x &\mapsto P(X \le x)
    \end{eqnarray*}
    that is
    \begin{eqnarray*}
        F_X(x) := P(\text{$X$ takes values less than or equal to $x$}) 
    \end{eqnarray*}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we toss a coin with $P(H) = p \in (0,1)$ once. Let $X$ be the random variable that tracks the outcome of the coint toss, that is 
    $$X(\omega) = \begin{cases}
        1 \quad \text{if $\omega = H$}\\
        0 \quad \text{if $\omega = T$}
    \end{cases}$$
    We now calculate the cumulative distribution function for $X$. 

    \begin{enumerate}
        \item If $x < 0$:
\begin{eqnarray*}
\{X \leq x\} &=& \{\text{event that is impossible}\} \\
&=& \emptyset \\
\end{eqnarray*}
Therefore we must have:
\begin{eqnarray*}
    F_X(x) &=& P(X \leq x)\\
    &=& P(\emptyset) \\
    &=& 0
\end{eqnarray*}
for all $x <0$. 
\item If $x = 0$, 
\begin{eqnarray*}
    F_X(0) &=&  P(X \leq 0) \\
&=& \underbrace{P(X < 0)}_{0} + \underbrace{P(X = 0)}_{P(T)=(1-p)} 
\end{eqnarray*}
Therefore,
$$F_X(0) = 1-p$$
\item If $x \in (0,1)$,
\begin{eqnarray*}
    F_X(x) &=& P(X \leq x) \\
&=& \underbrace{P(X \leq 0)}_{0} + \underbrace{P(X=0)}_{1-p} + \underbrace{P(0<X<x<1)}_{0} \\
\end{eqnarray*}
Therefore, 
$$F_X(x) = (1-p) \quad \text{for all } x \in (0,1)$$
\item If $x=1$, 
\begin{eqnarray*}
    F_X(1) &=&  P(X \leq 1) \\
&=& \underbrace{P(X < 1)}_{1-p} + \underbrace{P(X = 1)}_{P(H)=p} 
\end{eqnarray*}
Therefore, 
$$F_X(1) = 1 $$
\item If $x>1$, 
\begin{eqnarray*}
    F_X(x) &=&  P(X \leq x) \\
&=& \underbrace{P(X \le 1)}_{1} + \underbrace{P(X > 1)}_{0} 
\end{eqnarray*}
Therefore, 
$$F_X(x) = 1 \quad \text{for all } x > 1 $$

    \end{enumerate}



The cdf $F_X$ can be written as follows:

\[
F_X(x) = \begin{cases}
0 & \text{if } x < 0 \\
(1-p) & \text{if } x \in [0,1) \\
1 & \text{if } x \geq 1
\end{cases}
\]

The plot of $F_X(x)$:
\vspace{1em}

\begin{tikzpicture}
    % Axis
    \draw[thick, ->] (-1.5,0) -- (3,0) node[anchor=north] {$x$};
    \draw[thick, ->] (0,-0.5) -- (0,3) node[anchor=east] {$F_X(x)$};
    \draw[very thin, gray] (-1.5,-0.5) grid (3,3);

    % CDF steps
    \draw[blue, thick] (-1.5, 0) -- (0, 0);
    \draw[orange, thick] (0, 0) -- (0, 1.5);
    \draw[blue, thick] (0, 1.5) -- (2, 1.5);
    \draw[orange, thick] (2, 1.5) -- (2, 3);
    \draw[blue, thick] (2, 3) -- (3, 3);

    % Points
    \filldraw[white, draw=blue, thick] (0, 0) circle (2pt);
    \filldraw[blue] (0, 1.5) circle (2pt);
    \filldraw[white, draw=blue, thick] (2, 1.5) circle (2pt);
    \filldraw[blue] (2, 3) circle (2pt);

    % Labels
    % \node at (-0.1, 0.15) [anchor=east] {$0$};
    \node at (0, 1.2) [anchor=west] {$1-p$};
    \node at (2, 2) [anchor=west] {$p$};
    \node at (0, 3) [anchor=west] {$1$};
    \node at (0, -0.3) [anchor=north] {$0$};
    \node at (2, -0.3) [anchor=north] {$1$};

    % Braces and annotations
    % \draw[decorate, decoration={brace, amplitude=10pt}] (-0.1,3) -- (-0.1, 1.5) node [black, midway, xshift=-0.6cm] {$p$};

\end{tikzpicture}"
Random Variables,Random Variables: Examples and Definitions,ex,"Suppose we toss a fair coin ($P(H) = \frac{1}{2}$) three times, and let 
$$X(\omega) := \text{number of heads in $\omega$}$$
Values that $X$ can take are 
$$ \X \coloneq \{0,1,2,3\}$$ 
\\

We now calculate the cdf of $X$.

\begin{enumerate}
    \item If $x < 0$:
\begin{eqnarray*}
F_X(x) &= P(X \leq x) \\
&= 0
\end{eqnarray*}
Therefore 
$$ F_X(x) = 0 \quad \text{for $x<0$}$$

\item If $x = 0$:
\begin{eqnarray*}
F_X(x) &=& P(X \leq 0) \\
&=& \underbrace{P(X < 0)}_{0} + \underbrace{P(X = 0)}_{\underbrace{P(\text{zero heads in three tosses})}_{P(TTT) = (\frac{1}{2})^3}}\end{eqnarray*} 
Therefore  $$F_X(0) = \frac{1}{8}$$
\item If  $x \in (0,1)$:
\begin{eqnarray*}
F_X(x) &= P(X \leq x) \\
&= \underbrace{P(X \leq 0)}_{\frac{1}{8}} + \underbrace{P(0 < X \leq x)}_{0} 
\end{eqnarray*}
Therefore 
$$F_X(x) &= \frac{1}{8} \quad \text{for } x \in (0,1)$$
\item  If $x = 1$:
\begin{eqnarray*}
F_X(1) &= P(X \leq 1) \\
&=& \underbrace{P(X \leq 0)}_{\frac{1}{8}} + \underbrace{P(0 < X < 1)}_{0} + \underbrace{P(X = 1)}_{\underbrace{P(\text{exactly two heads in three tosses})}_{P(\{HTT,THT,TTH\})=\frac{3}{8}}} \\
&=& \frac{1}{8} + P(HTT, THT, TTH) = \frac{1}{8} + 3\left(\frac{1}{2}\right)^3 = \frac{1}{8} + \frac{3}{8}
\end{eqnarray*}

Therefore  
$$F_X(1) &= \frac{4}{8} = \frac{1}{2}$$

\item  If $x \in (1,2)$:
\begin{eqnarray*}
F_X(x) &=& P(X \leq x) \\
&=& \underbrace{P(X \leq 1)}_{\frac{1}{2}} + \underbrace{P(1 < X \leq x)}_{0} \\
\end{eqnarray*}
Therefore  
$$F_X(x) &= \frac{1}{2} \quad \text{for $x \in (1,2)$}$$

\item  If $x = 2$:
\begin{eqnarray*}
F_X(2) &= P(X \leq 2) \\
&=& \underbrace{P(X \leq 1)}_{\frac{1}{2}} + \underbrace{P(1 < X < 2)}_{0} + \underbrace{P(X = 2)}_{P(\text{exactly two heads in three tosses})} \\
&=& \frac{1}{2} + P(HHT, HTH, THH) = \frac{1}{2} + 3\left(\frac{1}{2}\right)^3 = \frac{1}{2} + \frac{3}{8} \\
\end{eqnarray*}
Therefore $$F_X(2) &= \frac{7}{8}$$

\item  If $x \in (2,3)$:
\begin{eqnarray*}
F_X(x) &= P(X \leq x) \\
&= \underbrace{P(X \leq 2)}_{\frac{7}{8}} + \underbrace{P(2 < X < 3)}_{0} \\
\end{eqnarray*}
Therefore
$$F_X(x) &= \frac{7}{8} \quad \text{for $x \in (2,3)$} $$

\item If  $x \geq 3$:
\begin{eqnarray*}
F_X(x) &=& P(X \leq x) \\
&=& \underbrace{P(X \leq 2)}_{\frac{7}{8}} + \underbrace{P(2 < X \leq 3)}_{\text{P(X = 3)}} \\
&=& \frac{7}{8} + P(HHH) = \frac{7}{8} + \left(\frac{1}{2}\right)^3 = \frac{7}{8} + \frac{1}{8} = 1
\end{eqnarray*}

\end{enumerate}

We can consolidate all the information about $F_X$ as follows:
\[
F_X(x) = \begin{cases}
0 & \text{if } x < 0 \\
\frac{1}{8} & \text{if } x \in [0,1) \\
\frac{1}{2} & \text{if } x \in [1,2) \\
\frac{7}{8} & \text{if } x \in [2,3) \\
1 & \text{if } x \geq 3
\end{cases}
\]

The plot of $F_X$ is given below

\vspace{1em}
\begin{center}
\begin{tikzpicture}[scale=1.5]
    % Define variables for y-coordinates
    \def\yscale{4}
    \def\ya{0.125*\yscale}
    \def\yb{0.5*\yscale}
    \def\yc{0.875*\yscale}
    \def\yd{1*\yscale}

    % Axis
    \draw[thick, ->] (-0.5,0) -- (3.5,0) node[anchor=north] {$x$};
    \draw[thick, ->] (0,-0.1) -- (0,\yscale+0.2) node[anchor=east] {$F_X(x)$};
    \draw[very thin, gray] (-0.5,-0.1) grid (3.5,1.2*\yscale);

    % CDF steps
    \draw[blue, thick] (-0.5, 0) -- (0, 0);
    \draw[orange, thick] (0, 0) -- (0, \ya);
    \draw[blue, thick] (0, \ya) -- (1, \ya);
    \draw[orange, thick] (1, \ya) -- (1, \yb);
    \draw[blue, thick] (1, \yb) -- (2, \yb);
    \draw[orange, thick] (2, \yb) -- (2, \yc);
    \draw[blue, thick] (2, \yc) -- (3, \yc);
    \draw[orange, thick] (3, \yc) -- (3, \yd);
    \draw[blue, thick] (3, \yd) -- (3.5, \yd);

    % Points
    \filldraw[white, draw=blue, thick] (0, 0) circle (1pt);
    \filldraw[blue] (0, \ya) circle (1pt);
    \filldraw[white, draw=blue, thick] (1, \ya) circle (1pt);
    \filldraw[blue] (1, \yb) circle (1pt);
    \filldraw[white, draw=blue, thick] (2, \yb) circle (1pt);
    \filldraw[blue] (2, \yc) circle (1pt);
    \filldraw[white, draw=blue, thick] (3, \yc) circle (1pt);
    \filldraw[blue] (3, \yd) circle (1pt);

    % Labels
    \node at (-0.1, \ya) [anchor=east] {$\frac{1}{8}$};
    \node at (0.9, \yb) [anchor=east] {$\frac{1}{2}$};
    \node at (1.9, \yc) [anchor=east] {$\frac{7}{8}$};
    \node at (-0.1, \yd) [anchor=east] {$1$};
    \node at (0, -0.1) [anchor=north] {$0$};
    \node at (1, -0.1) [anchor=north] {$1$};
    \node at (2, -0.1) [anchor=north] {$2$};
    \node at (3, -0.1) [anchor=north] {$3$};
\end{tikzpicture}
\end{center}"
Random Variables,Random Variables: Examples and Definitions,thm,"[Classification of CDFs]
A function $F(x)$ is a cumulative density function if and only if the following three conditions are satisifies: \begin{enumerate}
\item $\lim_{x\to -\infty}F(x)=0,$  $\lim_{x\to \infty}F(x)=1$
\item $F(x)$ is a non decreasing function
\item $F(x)$ is right continuous. i.e $\lim_{x\to x_o^+}F(x)=F(x_o),$ $\forall x_o\in \R$
\end{enumerate}"
Random Variables,Discrete Random Variables,defn,"[Discrete Random Variable]
    We say a random variable $X: \mathcal{S} \longrightarrow \mathbb{R}$ is \textbf{discrete} if the cumulative distribution function of $X$, $F_X$ is a step function."
Random Variables,Discrete Random Variables,defn,"[Probability Mass Function]
    If $X$ is a discrete random variable, the probability mass function associated to $X$ is defined as 
    \begin{eqnarray*}
        p_X :& \mathbb{R} &\longrightarrow \mathbb{R}\\
        & x &\mapsto P(X = x)
    \end{eqnarray*}
    that is, 
    $$p_X(x) := P(\text{$X$ takes the value $x$})$$"
Random Variables,Discrete Random Variables,defn,"[Parameters for Discrete Variables]
Suppose $X$ is a discrete random variable with probability mass function $p_X$ and taking values in the set $\mathcal{X}$. We can consider the following parameters associated to $X$
\begin{enumerate}
    \item \textbf{Expected Value:}
    $$E(X) := \mu_X = \sum_{x\in \mathcal{X}} x\; p_X(x)$$
    \item \textbf{Statisticians Unconscious Law:} Given a function $g: \mathbb{R} \longrightarrow \mathbb{R}$, then 
    $$E(g(X)) := \sum_{x\in \mathcal{X}} g(x) \; p_X(x)$$
    \item \textbf{Variance:}
    \begin{eqnarray*}
        V(X) := \sigma^2_X =& E ((X-\mu_X)^2)\\
        =& \sum_{x\in \mathcal{X}} (x-\mu_X)^2\; p_X(x)
    \end{eqnarray*}
    
\end{enumerate}"
Random Variables,Continuous Random Variables,defn,"[Continuous Random Variable]
We say a random variable $X: \mathcal{S} \longrightarrow \mathbb{R}$ is \textbf{continuous} if the cumulative distribution function of $X$, $F_X$ is a continuous function."
Random Variables,Continuous Random Variables,defn,"[Probability Density Function]
    We say that the function $f_X: \mathbb{R}\longrightarrow \mathbb{R}$ is the probability density function of the continuous random variable $X$ (with cdf $F_X$) if $f_X$ satisfies the following:
    $$ F_X(x) = \int_{-\infty}^x f_X(t) dt$$"
Random Variables,Continuous Random Variables,defn,"[Parameters for Continuous Variables]
    Suppose $X$ is a continuous random variable with probability density function $f_X$. We can consider the following parameters associated to $X$
\begin{enumerate}
    \item \textbf{Expected Value:}
    $$E(X) := \mu_X = \int_{-\infty}^\infty x\; f_X(x)\rm{dx}$$
    \item \textbf{Statisticians Unconscious Law:} Given a function $g: \mathbb{R} \longrightarrow \mathbb{R}$, then 
    $$E(g(X))  := \int_{-\infty}^\infty g(x)\; f_X(x)\rm{dx}$$
    \item \textbf{Variance:}
    \begin{eqnarray*}
        V(X) := \sigma^2_X =& E ((X-\mu_X)^2)\\
        =& \int_{-\infty}^\infty (x-\mu_X)^2\; f_X(x)\rm{dx}
    \end{eqnarray*}
\end{enumerate}"
Random Variables,Continuous Random Variables,defn,"[Identically Distributed Random Variables]
    We say two random variables $X$ and $Y$ are identically distributed if the corresponding cdfs $F_X$ and $F_Y$ are pointwise equal (almost) everywhere."
Random Variables,Continuous Random Variables,ex,"Suppose we toss a fair coin five times, and $X$ tracks the number of heads in five tosses, and $Y$ tracks the number of tails in five tosses. Then, it is possible to show that even though $X$ and $Y$ are not equal as functions on the sample space, the cdfs $F_X$ and $F_Y$ are exactly the same. As a result $X$ and $Y$ will be identically distributed. 
    \\

    Would we be able to conclude the same if we were to toss an unfair coin?"
Discrete Random Variables,Hypergeometric Distribution,defn,"[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$"
Discrete Random Variables,Hypergeometric Distribution,thm,"[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}"
Discrete Random Variables,Geometric Distribution,defn,"[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$"
Discrete Random Variables,Geometric Distribution,thm,"[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$"
Discrete Random Variables,Geometric Distribution,thm,"[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}"
Discrete Random Variables,Negative Binomial Distribution,defn,"[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$"
Discrete Random Variables,Negative Binomial Distribution,thm,"[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$"
Discrete Random Variables,Negative Binomial Distribution,thm,"[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}"
Discrete Random Variables,Poisson Distribution,defn,"[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$"
Discrete Random Variables,Poisson Distribution,thm,"[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}"
