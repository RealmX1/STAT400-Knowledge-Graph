timestamp,chapter_name,stage_name,node_name,system_prompt,user_input,response
1734088444.1189494,Chapter03,context2node-md,Variance of Binomial Random Variable,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">   node_type                                         node_name
0    concept                          Probability Distribution
1    concept                             Discrete Distribution
2    concept                          Discrete Random Variable
3    concept                   Probability Mass Function (PMF)
4    concept                     Uniform Discrete Distribution
5    concept                                   Bernoulli Trial
6    concept                            Bernoulli Distribution
7    concept                             Binomial Distribution
8    concept                       Hypergeometric Distribution
9    concept                            Geometric Distribution
10   concept                    Negative Binomial Distribution
11   concept                              Poisson Distribution
12   concept                    Expected Value of Distribution
13   concept                          Variance of Distribution
14  property            Expected value of Uniform Distribution
15  property                  Variance of Uniform Distribution
16  property                       PMF of Uniform Distribution
17  property          Expected value of Bernoulli Distribution
18  property                Variance of Bernoulli Distribution
19  property                     PMF of Bernoulli Distribution
20  property           Expected value of Binomial Distribution
21  property                 Variance of Binomial Distribution
22  property                      PMF of Binomial Distribution
23  property     Expected value of Hypergeometric Distribution
24  property           Variance of Hypergeometric Distribution
25  property                PMF of Hypergeometric Distribution
26  property          Expected value of Geometric Distribution
27  property                Variance of Geometric Distribution
28  property                     PMF of Geometric Distribution
29  property  Expected value of Negative Binomial Distribution
30  property        Variance of Negative Binomial Distribution
31  property             PMF of Negative Binomial Distribution
32  property                       PMF of Poisson Distribution
33  property            Expected value of Poisson Distribution
34  property                  Variance of Poisson Distribution
35  exercise                                         Coin Flip
36  exercise                             Multi-round Coin Flip
37  exercise                                     Fair Die Toss
38  exercise            Defective Widgets from Production Line</div>
  <div class=""node-name"">Variance of Binomial Random Variable</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Variance of Binomial Random Variable

## Attributes

- **Name**: Variance of Binomial Random Variable
  - **Aliases**: [""Variance of Binomial Distribution"", ""Var(Binomial)""]

- **Definition**:
  - **Notation**: 
    - Latex: `V(Binom(n, p)) = np(1-p)`
    - Rendered: $V(Binom(n, p)) = np(1-p)$
    - **Range**: $V(Binom(n, p)) \in [0, n]$

- **Description**: 
  The variance of a binomial random variable expresses the spread or variability of the number of successes in a fixed number of Bernoulli trials. It is a measure of how much the number of successes deviates from the expected value over repeated experiments.

- **Proof**: 
  The variance of a binomial distribution can be derived by using the formula for variance of a sum of independent random variables. Each trial in the binomial distribution is independent, and the trials can be considered as Bernoulli trials with variance $p(1-p)$. The variance of the binomial distribution is $np(1-p)$ since it is the sum of variances from these $n$ independent trials.
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Binomial Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Variance of Distribution

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - None relevant subproperties found within context and existing nodes.

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - None relevant prerequisite properties found within context and existing nodes.
```
"
1734088502.1364958,Chapter03,context2node-md,Expected value of Discrete Uniform Random Variable,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Expected value of Discrete Uniform Random Variable</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","# Expected value of Discrete Uniform Random Variable

## Attributes

- **Name**: Expected value of Discrete Uniform Random Variable
  - **Aliases**: [""Mean of Uniform Discrete Distribution"", ""Expected Value of Uniform Distribution""]

- **Definition**:
  - **Notation**: 
    - Latex: `E(X) = \frac{N+1}{2}`
    - Rendered: $E(X) = \frac{N+1}{2}$
    - **Range**: $E(X) \in \{1, \frac{N+1}{2}, N\}$

- **Description**: 
  - The expected value of a discrete uniform random variable is the average of the set of possible outcomes. For a uniform discrete distribution supported on $\{1, 2, 3, \dots, N\}$, the expected value is the midpoint of the range of possible outcomes, which is calculated as $\frac{N+1}{2}$.

- **Proof**: 
  - The expected value of a uniform discrete random variable $X$ supported on $\{1, 2, \dots, N\}$ with each outcome equally probable is given by the formula $E(X) = \frac{1+2+3+\cdots+N}{N} = \frac{N(N+1)/2}{N} = \frac{N+1}{2}$. 
  - *Source:* Stat400 teaching material - Jonathan Fernandez

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - **Concept**: Uniform Discrete Random Variable
  - The concept of Uniform Discrete Random Variable possesses this property.

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - **Concept**: Expected Value of random variable
  - The property is a child concept of ""Expected Value of random variable"".

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*"
1734088550.3278098,Chapter03,context2node-md,PMF of Discrete Uniform Random Variable,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Discrete Uniform Random Variable</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: PMF of Discrete Uniform Random Variable
  - **Aliases**: [""Probability Mass Function of Discrete Uniform Distribution"", ""Discrete Uniform PMF""]

- **Definition**: The probability mass function (pmf) for a discrete uniform random variable $X$ with parameter $N \in \N$ is given by:
  - **Notation**: 
    - Raw Latex: `p_X(x) = \frac{1}{N} \quad \text{for all } x \in \X`
    - Rendered: $p_X(x) = \frac{1}{N} \quad \text{for all } x \in \mathcal{X}$
  - **Range**: The domain of $X$ is $\mathcal{X} = \{1, 2, 3, \cdots, N\}$.

- **Description**: A discrete uniform random variable is one where each of its finite possible values is equally likely. The pmf is constant across the support of the distribution.

- **Proof**: *Add proof here if necessary.*

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Uniform Discrete Random Variable (Concept)
  
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Probability Mass Function (Concept)
```

---

Note: If necessary, add any additional details or corrections separately."
1734088567.5276685,Chapter03,context2node-md,PMF of Binomial Random Variable,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Binomial Random Variable</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: PMF of Binomial Random Variable
  - **Aliases**: [""Probability Mass Function of Binomial Distribution"", ""Binomial PMF""]

- **Definition**: 
  - **Notation**: 
    - Latex: `p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}`
    - Rendered: $p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}$
    - **Range**: Dual interpretation as it covers $x \in \{0, 1, 2, \dots, n\}$.

- **Description**: 
  The probability mass function (PMF) of a Binomial random variable, $X \sim \text{Binomial}(n, p)$, gives the probability of obtaining exactly $x$ successes in $n$ independent Bernoulli trials, each with a probability of success $p$.

- **Proof**: 
  The proof involves explicitly calculating the PMF of $X$ by considering all possible sequences of successes and failures and summing their respective probabilities.

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* 
  - Binomial Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* 
  - Probability Mass Function 

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*

> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`
```"
1734088609.880839,Chapter03,md2json,Bernoulli Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => Discrete Distribution, Bernoulli Trial""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}},{""target_node"":{""target_node_name"":""Bernoulli Trial"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088610.5912373,Chapter03,md2json,Bernoulli Trial,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": """"}","{""relationships"":{""has_subconcept"":null,""is_prerequisite_of"":null}}"
1734088611.7674053,Chapter03,md2json,Binomial Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* \""Discrete Distribution\"" \n  - \""Binomial Distribution\"" is a subconcept of \""Discrete Distribution\""""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088612.6891854,Chapter03,md2json,Discrete Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution, Random Variable]\n  - The concept \""Discrete Random Variable\"" helps form the foundation for understanding discrete distributions.""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Random Variable"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088614.0086176,Chapter03,md2json,Discrete Uniform Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution] \n  - Discrete Distribution has_subconcept Discrete Uniform Distribution\n""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Uniform Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088615.7644129,Chapter03,md2json,Expected Value of Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Random Variable]\n  - A parent concept has the current concept as a subconcept. Since \""Expected Value of Random Variable\"" is calculated for random variables, it naturally fits as a part of \""Random Variable\"".""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Random Variable"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088616.6377916,Chapter03,md2json,Probability Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": """"}","{""relationships"":{""has_subconcept"":null,""is_prerequisite_of"":null}}"
1734088617.9319296,Chapter03,md2json,Probability Mass Function (PMF),"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Random Variable]""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Random Variable"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088619.065226,Chapter03,md2json,Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Probability Distribution] \n  - Probability Distribution has Random Variable as a subconcept.""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Probability Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088620.0491996,Chapter03,md2json,Variance of Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Random Variable]\n  - \""Random Variable\"" has \""Variance of Random Variable\"" as a subconcept.""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Variance of Random Variable"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734088663.6429324,Chapter03,md2json,Expected value of Bernoulli Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Bernoulli Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Expected Value of Random Variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Bernoulli Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088665.3822238,Chapter03,md2json,Expected value of Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Binomial Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Expected Value of Random Variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088667.10789,Chapter03,md2json,Expected value of Discrete Uniform Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Uniform Discrete Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Expected Value of Random Variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Uniform Discrete Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088668.812215,Chapter03,md2json,PMF of Bernoulli Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - *Bernoulli Distribution*\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - *Probability Mass Function (PMF)*\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n```""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Bernoulli Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088670.7965808,Chapter03,md2json,PMF of Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept* \n  - Binomial Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* \n  - Probability Mass Function (PMF)\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n\n> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088673.078986,Chapter03,md2json,PMF of Discrete Uniform Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST\n  - Concept: Uniform Discrete Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST\n  - Concept: Probability Mass Function (PMF)\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n  - Subproperty: *None explicitly listed*\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n  - Depends on: *None explicitly listed*\n\n> **Source**: Stat400 teaching material - Jonathan Fernandez""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Uniform Discrete Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088675.0339246,Chapter03,md2json,Variance of Bernoulli Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) Bernoulli Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) Variance of Random Variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Bernoulli Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088676.9766564,Chapter03,md2json,Variance of Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Binomial Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Variance of Random Variable\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n  - None relevant subproperties found within context and existing nodes.\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n  - None relevant prerequisite properties found within context and existing nodes.""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734088678.707733,Chapter03,md2json,Variance of Discrete Uniform Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - **Discrete Uniform Distribution**\n  \n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - **Variance of Random Variable**""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Discrete Uniform Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734096807.9762402,Chapter03,context2node-md,Hypergeometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Hypergeometric Distribution</div>
  <div class=""node-type"">concept</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Concept Node Schema

## Attributes

- **Name**: *String*
  - Uniform discrete distribution
  - **Aliases**: *List of Strings*
    - Synonyms or alternative names for the concept. 
    - *Example*: Poisson Distribution
- **Definition**:
  - **Notation**: *String* (Latex)
    - Symbols or mathematical notations associated with the concept. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $Pois(\lambda)$
      - Latex: `Pois(\lambda)` 
  - **Explanation**: *List of Objects*
    - Disect the notation and explain each part. Should have range (if applicable) and description for each respective part.
    - *Example*:
      - $\lambda$ 
        - Range: $\lambda \in \{0, \infty\}$
        - Description:represent average rate or mean number of occurrences of the event being modeled. 
      - $other$
        - Description: More information parts of the notation that need explanation if necessary.
  - **Example**:
    - $$p_X(x) = \begin{cases}
      1-p \quad \text{if $x=0$}\\
      p \quad \text{if $x=1$}
  \end{cases}$$ (this example is for the concept node ""Probability Mass Function (PMF)""; using bernoulli distribution as an example)
      - Description: Represents a discrete random variable, which is a function that maps each outcome of a random experiment to a real number.
- **Description**: *Text*
  - Additional explanations, background information, or context about the concept.
  - *Example*: ""Poisson Distribution is a discrete probability distribution that describes the number of times an event occurs within a fixed interval of time or space. It assumes that events occur with a constant mean rate and are independent of the time since the last event.""



## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [parent concept name] 
  - A parent concept has current concept as a subconcept.
  - *Example*: ""Poisson Distribution"" should log relation ""has_subconcept"" from ""Discrete Distribution""; i.e., ""Discrete Distribution"" has_subconcept ""Poisson Distribution""


</div>
</div>","```markdown
# Hypergeometric Distribution

## Attributes

- **Name**: Hypergeometric Distribution
  - **Aliases**: 
    - Hyper Dist

- **Definition**:
  - **Notation**: 
    - Raw LaTeX: `X \sim \text{Hyper}(N, M, n)`
    - Rendered: $X \sim \text{Hyper}(N, M, n)$
  - **Explanation**:
    - $N$
      - Range: $N \in \mathbb{N}$
      - Description: Total population size
    - $M$
      - Range: $M \in \mathbb{N}$
      - Description: Number of successes in the population
    - $n$
      - Range: $n \in \mathbb{N}$
      - Description: Number of individuals chosen from the population
  - **Example**:
    - $$p_X(x) = \frac{{M \choose x} {N-M \choose n-x}}{{N \choose n}}$$
      - Description: The pmf describes the probability of finding exactly $x$ successes in the sample when sampling without replacement.

- **Description**: 
  - The Hypergeometric Distribution models the number of successes in a sequence of $n$ draws from a finite population of size $N$ containing exactly $M$ successes, without replacement.

## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Random Variables] 
```

### Notes
- No existing nodes are present, so no relationships could be established using actual node names. Ensure to check for possible connections when more nodes are present.
- The LaTeX formatting for hypergeometric distribution was corrected to align with the schema.
- Definitions and descriptions were added based on context from the provided chapter and italicized if additional context was inferred."
1734097212.81293,Chapter03,context2node-md,Geometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Geometric Distribution</div>
  <div class=""node-type"">concept</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Concept Node Schema

## Attributes

- **Name**: *String*
  - Uniform discrete distribution
  - **Aliases**: *List of Strings*
    - Synonyms or alternative names for the concept. 
    - *Example*: Poisson Distribution
- **Definition**:
  - **Notation**: *String* (Latex)
    - Symbols or mathematical notations associated with the concept. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $Pois(\lambda)$
      - Latex: `Pois(\lambda)` 
  - **Explanation**: *List of Objects*
    - Disect the notation and explain each part. Should have range (if applicable) and description for each respective part.
    - *Example*:
      - $\lambda$ 
        - Range: $\lambda \in \{0, \infty\}$
        - Description:represent average rate or mean number of occurrences of the event being modeled. 
      - $other$
        - Description: More information parts of the notation that need explanation if necessary.
  - **Example**:
    - $$p_X(x) = \begin{cases}
      1-p \quad \text{if $x=0$}\\
      p \quad \text{if $x=1$}
  \end{cases}$$ (this example is for the concept node ""Probability Mass Function (PMF)""; using bernoulli distribution as an example)
      - Description: Represents a discrete random variable, which is a function that maps each outcome of a random experiment to a real number.
- **Description**: *Text*
  - Additional explanations, background information, or context about the concept.
  - *Example*: ""Poisson Distribution is a discrete probability distribution that describes the number of times an event occurs within a fixed interval of time or space. It assumes that events occur with a constant mean rate and are independent of the time since the last event.""



## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [parent concept name] 
  - A parent concept has current concept as a subconcept.
  - *Example*: ""Poisson Distribution"" should log relation ""has_subconcept"" from ""Discrete Distribution""; i.e., ""Discrete Distribution"" has_subconcept ""Poisson Distribution""


</div>
</div>","```markdown
# Concept Node: Geometric Distribution

## Attributes

- **Name**: Geometric Distribution
  - **Aliases**: 
    - *None provided*

- **Definition**:
  - **Notation**: 
    - Latex: `Geom(p)`
    - Rendered: $Geom(p)$
  - **Explanation**:
    - $p$
      - Range: $p \in (0, 1)$
      - Description: Represents the probability of success in a Bernoulli trial.
    - $other$
      - Description: None

  - **Example**:
    - ```latex
      p_X(x) = p^x(1-p)^{(x-1)} \quad \quad x\in \{1, 2, 3, \dots, \}
      ```
      - Description: This is the probability mass function for the Geometric distribution, which describes the probability of observing the first success on the $x$-th trial.

- **Description**: The geometric distribution models phenomena involving ""waiting"" for the first success in a sequence of independent Bernoulli trials. It is the discrete probability distribution of the number of trials needed to get one success. The geometric distribution assumes that each trial is an independent Bernoulli trial with the same probability of success.

## Relationships

- **is_subconcept_of** => Discrete Distribution
```"
1734097808.0001512,Chapter03,context2node-md,Negative Binomial Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Negative Binomial Distribution</div>
  <div class=""node-type"">concept</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Concept Node Schema

## Attributes

- **Name**: *String*
  - Uniform discrete distribution
  - **Aliases**: *List of Strings*
    - Synonyms or alternative names for the concept. 
    - *Example*: Poisson Distribution
- **Definition**:
  - **Notation**: *String* (Latex)
    - Symbols or mathematical notations associated with the concept. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $Pois(\lambda)$
      - Latex: `Pois(\lambda)` 
  - **Explanation**: *List of Objects*
    - Disect the notation and explain each part. Should have range (if applicable) and description for each respective part.
    - *Example*:
      - $\lambda$ 
        - Range: $\lambda \in \{0, \infty\}$
        - Description:represent average rate or mean number of occurrences of the event being modeled. 
      - $other$
        - Description: More information parts of the notation that need explanation if necessary.
  - **Example**:
    - $$p_X(x) = \begin{cases}
      1-p \quad \text{if $x=0$}\\
      p \quad \text{if $x=1$}
  \end{cases}$$ (this example is for the concept node ""Probability Mass Function (PMF)""; using bernoulli distribution as an example)
      - Description: Represents a discrete random variable, which is a function that maps each outcome of a random experiment to a real number.
- **Description**: *Text*
  - Additional explanations, background information, or context about the concept.
  - *Example*: ""Poisson Distribution is a discrete probability distribution that describes the number of times an event occurs within a fixed interval of time or space. It assumes that events occur with a constant mean rate and are independent of the time since the last event.""



## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [parent concept name] 
  - A parent concept has current concept as a subconcept.
  - *Example*: ""Poisson Distribution"" should log relation ""has_subconcept"" from ""Discrete Distribution""; i.e., ""Discrete Distribution"" has_subconcept ""Poisson Distribution""


</div>
</div>","# Negative Binomial Distribution Node

## Attributes

- **Name**: Negative Binomial Distribution

- **Aliases**: 
  - NegBinom Distribution
  - Pascal Distribution

- **Definition**:
  - **Notation**: 
    - Latex: `NegBinom(r, p)` 
    - Rendered: $X \sim \text{NegBinom}(r, p)$
  - **Explanation**: 
    - $r$
      - Range: $r \in \mathbb{N}$
      - Description: The number of successes we are waiting for.
    - $p$
      - Range: $p \in (0, 1)$
      - Description: The probability of a success in each trial.
  - **Example**:
    - $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
      - Description: The probability mass function (pmf) of the negative binomial distribution, representing the probability of observing $x$ failures before achieving $r$ successes.

- **Description**: 
  - The Negative Binomial Distribution models phenomena that involve ""waiting"" for a specified number of successes in a sequence of independent and identically distributed Bernoulli trials. It calculates probabilities in terms of counting the number of failures preceding these successes. 

## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution] 
  - A parent concept ""Discrete Distribution"" has current concept ""Negative Binomial Distribution"" as a subconcept."
1734097825.2252524,Chapter03,context2node-md,Poisson Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Poisson Distribution</div>
  <div class=""node-type"">concept</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Concept Node Schema

## Attributes

- **Name**: *String*
  - Uniform discrete distribution
  - **Aliases**: *List of Strings*
    - Synonyms or alternative names for the concept. 
    - *Example*: Poisson Distribution
- **Definition**:
  - **Notation**: *String* (Latex)
    - Symbols or mathematical notations associated with the concept. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $Pois(\lambda)$
      - Latex: `Pois(\lambda)` 
  - **Explanation**: *List of Objects*
    - Disect the notation and explain each part. Should have range (if applicable) and description for each respective part.
    - *Example*:
      - $\lambda$ 
        - Range: $\lambda \in \{0, \infty\}$
        - Description:represent average rate or mean number of occurrences of the event being modeled. 
      - $other$
        - Description: More information parts of the notation that need explanation if necessary.
  - **Example**:
    - $$p_X(x) = \begin{cases}
      1-p \quad \text{if $x=0$}\\
      p \quad \text{if $x=1$}
  \end{cases}$$ (this example is for the concept node ""Probability Mass Function (PMF)""; using bernoulli distribution as an example)
      - Description: Represents a discrete random variable, which is a function that maps each outcome of a random experiment to a real number.
- **Description**: *Text*
  - Additional explanations, background information, or context about the concept.
  - *Example*: ""Poisson Distribution is a discrete probability distribution that describes the number of times an event occurs within a fixed interval of time or space. It assumes that events occur with a constant mean rate and are independent of the time since the last event.""



## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [parent concept name] 
  - A parent concept has current concept as a subconcept.
  - *Example*: ""Poisson Distribution"" should log relation ""has_subconcept"" from ""Discrete Distribution""; i.e., ""Discrete Distribution"" has_subconcept ""Poisson Distribution""


</div>
</div>","# Poisson Distribution Node

## Attributes

- **Name**: Poisson Distribution
  - **Aliases**: 
    - Synonyms: None specified in the content.

- **Definition**:
  - **Notation**: 
    - Latex: `Pois(\lambda)`
    - Rendered: $Pois(\lambda)$
  - **Explanation**: 
    - $\lambda$
      - Range: $\lambda \in \mathbb{R}_{+}$
      - Description: Represents the rate parameter, average rate, or mean number of occurrences of the event being modeled.
    - $other$
      - Description: $X \sim \text{Pois}(\lambda)$ means that $X$ has the Poisson distribution with the rate parameter $\lambda$.

  - **Example**:
    - $$p_X(x) = e^{-\lambda} \frac{\lambda^x}{x!}$$
      - Description: The probability mass function (PMF) provides the probability that a given number of events occurs in a fixed interval of time or space.

- **Description**: 
  - Poisson Distribution is a discrete probability distribution that describes the number of events occurring within a fixed interval of time or space. It is applicable when events occur independently, with a known constant mean rate.


## Relationships

- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Random Variable]
  - Explanation: The Poisson Distribution is a type of discrete random variable distribution used to model the probability of a number of events occurring within a specific timeframe.
"
1734097876.4582198,Chapter03,context2node-md,Expected value of Hypergeometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Expected value of Hypergeometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node: Expected value of Hypergeometric Distribution

## Attributes

- **Name**: Expected Value of Hypergeometric Distribution
  - **Aliases**: [""Expected Value"", ""Mean of Hypergeometric Distribution""]

- **Definition**: 
  Let $X \sim \text{Hyper}(N, M, n)$, where $N$ is the population size, $M$ is the number of successes in the population, and $n$ is the number of individuals chosen from the population.
  - **Notation**: 
    - Raw LaTeX: `E[Hyper(N, M, n)] = n \left(\frac{M}{N}\right)`
    - Rendered: $E[Hyper(N, M, n)] = n \left(\frac{M}{N}\right)$
  - **Range**: 
    - Raw LaTeX: `E[Hyper(N, M, n)] \in [0, min(n, M)]`
    - Rendered: $E[Hyper(N, M, n)] \in [0, \min(n, M)]$

- **Description**: 
  The expected value of the Hypergeometric Distribution is the average number of successes observed when $n$ individuals are chosen from a population of size $N$ containing $M$ successes without replacement. It captures the average outcome in terms of success count in simple random sampling from a finite population.

- **Proof**: 
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`
  *Proof*: 
  The expectation of a Hypergeometric random variable can be derived using the linearity of expectation and considering it as a sum of indicators for success in each of the $n$ draws. Each draw has a probability of success $\frac{M}{N}$. Hence, the expected number of successes is the product of the number of draws and the probability of success in each draw: 
  \[
  E(X) = n \cdot \frac{M}{N}
  \]

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Hypergeometric Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - Expected Value of Random Variable

```"
1734097953.2976437,Chapter03,context2node-md,Variance of Hypergeometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Variance of Hypergeometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: Variance of Hypergeometric Distribution
  - **Aliases**: [""Variance"", ""Var[X]"", ""Variance of Hyper-distribution""]

- **Definition**: For a hypergeometric random variable $X \sim \text{Hyper}(N, M, n)$.
  - **Notation**: 
    - Latex: `V[X] = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right)`
    - Rendered: $V[X] = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right)$
    - **Range**: \(V[X] \geq 0\)

- **Description**: Variance of the Hypergeometric Distribution measures the spread of the number of successes in the sampled group compared to what is expected on average. This is impacted by the distribution of successes in the overall population, the population size, and the sample size.

- **Proof**: *The variance is derived using the formula for variance of a discrete random variable.*

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - **Target**: Hypergeometric Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - **Target**: Variance of random variable
```"
1734098052.1612506,Chapter03,context2node-md,PMF of Hypergeometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Hypergeometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","# PMF of Hypergeometric Distribution

## Attributes

- **Name**: PMF of Hypergeometric Distribution
  - **Aliases**: Probability Mass Function of Hypergeometric Distribution

- **Definition**: 
  - **Notation**: 
    - Rendered: \( p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}} \)
    - Latex: `p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}` 
    - **Range**: \( x \in \{0, 1, 2, \dots, \min(n, M)\} \)

- **Description**: 
  The probability mass function (PMF) of the Hypergeometric Distribution gives the probability of observing exactly \(x\) successes in \(n\) draws from a population of size \(N\) containing exactly \(M\) successes, where the draws are made without replacement.

- **Proof**: 
  Refer to `Stat400 teaching material - Jonathan Fernandez`. Proof involves understanding the calculation of combinations for successful and unsuccessful outcomes, dividing by the total combinations for drawing \(n\) items from \(N\).

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*: Hypergeometric Distribution
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*: Probability Mass Function of a Discrete Random Variable
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*: N/A
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*: N/A

---

*Additional Notes:* 
- The PMF of the Hypergeometric Distribution is crucial for understanding events that involve sampling without replacement.
- This conceptual understanding also highlights the differences from similar discrete distributions like the Binomial Distribution, which assumes replacement."
1734098109.7022064,Chapter03,context2node-md,Expected value of Geometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Expected value of Geometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node: Expected Value of Geometric Distribution

## Attributes

- **Name**: Expected Value of Geometric Distribution
  - **Aliases**: [""Mean of Geometric Distribution""]

- **Definition**: 
  - **Notation**: 
    - Latex: `E[Geom(p)] = \frac{1}{p}`
    - Rendered: \(E[Geom(p)] = \frac{1}{p}\)
    - **Range**: 
      - Latex: `E[Geom(p)] \in [1, \infty)`
      - Rendered: \(E[Geom(p)] \in [1, \infty)\)

- **Description**: 
  The expected value of a geometric distribution represents the average number of trials needed to obtain the first success in a series of Bernoulli trials. It depends on the probability of success \(p\).

- **Proof**: 
  To find the expected value, consider the series of Bernoulli trials where the first success comes with probability \(p\) on the \(k\)-th trial. Therefore, the probability of failure in the previous trials is \((1-p)^{k-1}\).
  Using the equation for expected value of a random variable, and solving the geometric series, we obtain the expected value as \(\frac{1}{p}\).

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*: Geometric Distribution
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*: Expected Value
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*:  
  *(No subproperties specified)*
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*:  
  *(No specific dependencies provided)*
```
"
1734098152.489251,Chapter03,context2node-md,Variance of Geometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Variance of Geometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","# Variance of Geometric Distribution

## Attributes

- **Name**: Variance of Geometric Distribution
  - **Aliases**: [""Variance"", ""Var(X) of Geometric Distribution""]

- **Definition**: 
  - **Notation**: 
    - Latex: `V(X) = \frac{1-p}{p^2}`
    - Rendered: $V(X) = \frac{1-p}{p^2}$
    - **Range**: $V(X) \geq 0$

- **Description**: This property describes the variance of a geometric distribution, which represents the variability or spread in the number of trials needed to get the first success, with each trial being independent and having a probability of success $p$.

- **Proof**: *Step by step proof of this property would involve calculating $V(X) = E(X^2) - [E(X)]^2$, starting by deriving expressions for $E(X)$ and $E(X^2)$ for the geometric distribution.*

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) 
  - *Geometric Distribution*

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) 
  - *Variance of Random Variable*

- **has_subproperty** => (i.e. **is_subproperty_of** <=)

- **is_prerequisite_of** <= (i.e. **depends_on** =>) 
  - *Expected Value of Geometric Distribution*

## Additional Information

- **Source**: Information pertaining to the variance of the geometric distribution is based on standard statistical teaching materials like `Stat400 teaching material - Jonathan Fernandez`. 

*Note*: The proof generally requires showing that the variance formula arises from the geometric series calculations for expectations in the geometric distribution context."
1734098216.4580636,Chapter03,context2node-md,PMF of Geometric Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Geometric Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node: PMF of Geometric Distribution

## Attributes

- **Name**: PMF of Geometric Distribution
  - **Aliases**: Probability Mass Function of Geometric Distribution

- **Definition**: 
  - **Notation**:
    - Latex: `p_X(x) = p^x(1-p)^{(x-1)}`
    - Rendered: $p_X(x) = p^x(1-p)^{(x-1)}$
    - **Range**: $x \in \{1, 2, 3, \dots\}$

- **Description**: 
  The PMF of a Geometric distribution represents the probability of observing the first success on the $x$th trial in a series of independent Bernoulli trials, each of which has a probability of success $p$. The distribution models waiting times until the first success.

- **Proof**: 
  The proof involves calculating the probability that the first $x-1$ trials are failures and the $x$-th trial is a success. Hence, the probability is given by $p_X(x) = (1-p)^{x-1}p$. This can be shown by considering the sample space for the geometric experiment, which involves independent Bernoulli trials, with trials succeeding after $x-1$ failures. 
  > SOURCE: Stat400 teaching material - Jonathan Fernandez

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Geometric Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - Probability Mass Function

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
```"
1734098260.606479,Chapter03,context2node-md,Expected value of Negative Binomial Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Expected value of Negative Binomial Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: Expected Value of Negative Binomial Distribution

- **Definition**: 
  - **Notation**: 
    - Latex: `E[NegBinom(r, p)] = \frac{r(1-p)}{p}`
    - Rendered: $E[NegBinom(r, p)] = \frac{r(1-p)}{p}$
  - **Range**: 
    - \(E[NegBinom(r, p)] \in \mathbb{R}_{+}\)

- **Description**: 
  - The expected value of the Negative Binomial Distribution represents the average number of failures before observing the r-th success in a sequence of independent Bernoulli trials, each with a probability of success \(p\).

- **Proof**: 
  > NEED TO mark SOURCE for the information used here. By default, we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Negative Binomial Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Expected Value of Random Variable

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - 

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  -
```
"
1734098285.769519,Chapter03,context2node-md,Variance of Negative Binomial Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Variance of Negative Binomial Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: Variance of Negative Binomial Distribution
  - **Aliases**: [""Variance"", ""Second Moment of Negative Binomial Distribution""]

- **Definition**: 
  - **Notation**: 
    - Latex: `V(X) = \frac{r(1-p)}{p^2}`
    - Rendered: $V(X) = \frac{r(1-p)}{p^2}$

- **Description**: 
  - In the context of the Negative Binomial Distribution, the variance indicates the variability in the number of failures before achieving a predetermined number of successes. This measure informs about how dispersed the trial failures are in different sample scenarios.

- **Proof**:
  - *Proof involves deriving the variance using the properties of the Negative Binomial distribution and known results for variance calculations in negative binomial settings.*
  - *Observe: Variance results from calculating the squared deviation of the number of failures from the mean number of failures before the r-th success.*

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) Negative Binomial Distribution
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) Variance of random variable
```

No additional information from the context was omitted for this property node, and thus, there is no separate notes section needed."
1734098341.3383744,Chapter03,context2node-md,PMF of Negative Binomial Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Negative Binomial Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: PMF of Negative Binomial Distribution
  - **Aliases**: [""Probability Mass Function of Negative Binomial Distribution""]

- **Definition**: The PMF of the Negative Binomial Distribution, typically denoted by $X \sim \text{NegBinom}(r, p)$ is given by the formula:
  - **Notation**:
    - Latex: `p_X(x) = {x+r-1 \choose x}p^r(1-p)^x` 
    - Rendered: $p_X(x) = {x+r-1 \choose x}p^r(1-p)^x$
    - **Range**: $x\in \{0,1, 2, 3, \dots \}$
  
- **Description**: The probability mass function for a Negative Binomial distribution gives the probability of $X$ number of failures before achieving $r$ successes in a series of independent Bernoulli trials, where each trial has a probability $p$ of success.

- **Proof**:
  > To determine the PMF, consider the sample space consisting of all possible sequences of failures and successes, stopping at exactly the $r$th success. The binomial coefficient counts the number of ways to arrange $x$ failures among the trials needed to achieve $r$ successes. This PMF thus embodies both the sequence's ordering and the fixed number of successes needed.
  > 
  > *SOURCE: Stat400 teaching material - Jonathan Fernandez*

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - The **Negative Binomial Distribution** has_property the current property.

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - The current property ""PMF of Negative Binomial Distribution"" is a subconcept of ""Probability Mass Function""; ""Probability Mass Function"" has_subconcept the current property.
```"
1734098384.3808734,Chapter03,context2node-md,PMF of Poisson Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">PMF of Poisson Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node: PMF of Poisson Distribution

## Attributes

- **Name**: PMF of Poisson Distribution
  - **Aliases**: [""Probability Mass Function of Poisson Distribution""]

- **Definition**: 
  - **Notation**: 
    - LaTeX: `p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}` 
    - Rendered: $p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$
    - **Range**: $p_X(x) \in [0, 1]$ for $x \in \mathbb{N}$ or $x = 0, 1, 2, \dots$

- **Description**: The Probability Mass Function (PMF) of the Poisson Distribution gives the probability of observing exactly $k$ events in an interval, given the average or expected number of event occurrences $\lambda$.

- **Proof**: *The validation of this as a probability mass function can be seen via the series expansion of the exponential function:*

  $$e^\lambda = \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

  *Thus, the sum of probabilities over all possible $k$ equals 1.*

  > SOURCE: Stat400 teaching material - Jonathan Fernandez

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Poisson Distribution

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Probability Mass Function

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - *None presently defined*

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - *Not directly related in given context*
```"
1734098406.918369,Chapter03,context2node-md,Expected value of Poisson Distribution,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Expected value of Poisson Distribution</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node

## Attributes

- **Name**: Expected value of Poisson Distribution
  - **Aliases**: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: The expected value of a Poisson distributed random variable
  - **Notation**:
    - Latex: `E(X) = \lambda`
    - Rendered: $E(X) = \lambda$
    - **Range**: $E(X) \in \{0, \infty\}$

- **Description**: The expected value of a Poisson distribution indicates the average number of events in the given interval. It is equal to the parameter $\lambda$ of the distribution which represents the rate at which events occur.

- **Proof**: The expected value of a Poisson random variable can be demonstrated by calculating the sum of $k \cdot P(X = k)$ for all $k$, which simplifies to $\lambda$. [Source: Stat400 teaching material - Jonathan Fernandez]

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Poisson Distribution
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Expected value of random variable
```
"
1734098556.5553334,Chapter03,context2node-md,Variance of Poisson Random Variable,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Variance of Poisson Random Variable</div>
  <div class=""node-type"">property</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Property Node Schema

## Attributes

- **Name**: *String*
  - The official name of the property.
  - *Example*: ""Expected Value of Poisson Distribution""
  - **Aliases**: *List of Strings* (optional)
    - Alternative names or synonyms for the property.
    - *Example*: [""Expected Value"", ""Mean of Poisson Distribution""]

- **Definition**: *Text*
  - **Notation**: *String/Text* (LaTeX)
    - Symbols or mathematical notations associated with the property in latex format. First present raw latex code, then present the rendered latex code.
    - *Example*: 
      - Rendered: $E[Pois(\lambda)] = \lambda$
      - Latex: `E[Pois(\lambda)] = \lambda` 
    - **Range**: *String/Text* (LaTeX) (?)
      - The range of the property.
      - *Example*: $E[Pois(\lambda)] \in \{0, \infty\}$


- **Description**: *Text* (optional)
  - Additional explanations, background information, or context about the property.
  - *Example*: ""Expected Value of Poisson Distribution is the average number of times an event occurs within a fixed interval of time or space. It is the same as the lambda parameter of the Poisson Distribution.""

- **Proof**: *Text* (optional)
  - Step by step proof of the property. #TODO: Provide concrete example for sake of more consistenet LLM output
  > NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

  
## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST
  - Concepts or theorems that possess this property.
  - *Example*: Current Property ""Expected Value of Poisson Distribution"" is a property of ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.
- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST
  - The concept that this property is derived from/is a subset of.
  - *Example*: Current Property ""Variance of Poisson Distribution"" is a child concept of ""Variance of random variable""; ""Variance of Random Variable"" has_subconcept current property.
- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - More specific properties derived from the current property. The target property (subproperty) implies the current property.
  - *Example*: ""Strict Positivity"" (current property) has ""Non-negativity"" (target property) as a subproperty
- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Properties that are prerequisites for this property.
  - *Example*: Current Property ""Variance of Poisson Distribution"" depends on ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.

</div>
</div>","```markdown
# Property Node: Variance of Poisson Random Variable

## Attributes

- **Name**: Variance of Poisson Random Variable
  - **Aliases**: [""Variance of Poisson Distribution""]

- **Definition**: The variance of a Poisson random variable is the same as its rate parameter.
  - **Notation**: 
    - Rendered: $V[X \sim \text{Pois}(\lambda)] = \lambda$
    - LaTeX: `V[X \sim \text{Pois}(\lambda)] = \lambda`
    - **Range**: $V[X \sim \text{Pois}(\lambda)] \in \mathbb{R}_{0+}$

- **Description**: 
  - ""Variance of Poisson Random Variable quantifies the spread or dispersion of a Poisson-distributed data set. In the Poisson distribution, this value is equal to the expected rate or the lambda parameter.""

- **Proof**: 
  ""To prove that the variance of a Poisson random variable $X \sim \text{Pois}(\lambda)$ is $\lambda$, we can use the definition of variance, $V(X) = E[X^2] - (E[X])^2$. Since $E[X] = \lambda$ for a Poisson distribution, we need to derive $E[X^2]$. By utilizing moment generating functions or using the law of total probability with the fact that $X$ is equal to its expected value, $E[X^2] = \lambda^2 + \lambda$ can be calculated.”

## Relationships

- **has_property** <= (i.e. **is_property_of** =>) *Concept*
  - Concept: ""Poisson Distribution""; ""Poisson Distribution"" has_property current property.

- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*
  - Concept: ""Variance of Random Variable""; ""Variance of Random Variable"" has_subconcept current property.

- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*
  - None

- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*
  - Property: ""Expected Value of Poisson Distribution""; ""Expected Value of Poisson Distribution"" is a prerequisite of current property.
```
"
1734098626.1362665,Chapter03,context2node-md,Coin Flip,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Coin Flip</div>
  <div class=""node-type"">exercise</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Exercise Node Schema

> 10/07 No need to look into this in detail for the moment;



## Attributes

- **Name**: *String*
  - A brief title or identifier for the exercise.
  - *Example*: ""Exercise 3.2.1 Poisson Distribution Shopper Passing Through Store Front""

- **Problem Statement**: *Text*
  - The text of the exercise or problem.
  - *Example*: ""Given events A and B, where P(A) = 0.3 and P(B) = 0.5, and P(A ∩ B) = 0.15, compute P(A|B).""
  - **Notation**: *String/Text* (LaTeX)
    - Mathematical notation used in the problem. Show LaTeX code and rendered LaTeX side by side.
    - *Example*: ""P(A ∩ B)"" $P(A \cap B)$
  - **Diagram/Illustration**: *Image*
    - Any visual aids that accompany the problem.
    - *Example*: Venn diagram illustrating events A and B.

- **Solution**: *Text*
  - A detailed solution to the exercise.
  - *Example*: ""Using the definition of conditional probability, \( P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0.15}{0.5} = 0.3 \).""
  - **Step-by-Step Explanation**: *List of Text*
    - Detailed steps explaining how the solution is obtained.
    - *Example*: [""Compute \( P(A \cap B) \)"", ""Use the formula for conditional probability"", ""Simplify the expression""]

- **Hints**: *List of Text* (optional)
  - Hints or guidance to help solve the exercise.
  - *Example*: [""Recall the definition of conditional probability"", ""Ensure that all probabilities are within [0,1]""]

- **Difficulty Level**: *String* (optional)
  - An indication of the difficulty level.
  - *Example*: ""Beginner""

- **Knowledge** (optional)
  - Specific or rare knowledge about the exercise that doesn't fit into other sections.
  - Will be represented in terms of properties with relation links.
  - This part might need further specification after labeling more nodes.

> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships (Edges)

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem*/*Property*/*Application Example* [list of concept/theorem/property/application names] 
  - A list of all nodes that the exercise is about. Concepts, theorems, properties, or application examples that the exercise is about.
  - *Example*: [""Conditional Probability"",]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*/*Theorem*/*Property* [list of concept/theorem/property names]
  - Involves concept/theorem/property, but not as built as an exercise for it.
  - *Example*: [""Joint Probability"", ""Marginal Probability"", ""Bayes' Theorem"", ""Non-negativity""]

- **related_to** <=> *Exercise* [list of exercise names] (optional)
  - Other exercises that are related. For helping student get similar (but different) exercise.
  - *Example*: [""Exercise 3.2.2 asdf"", ""Exercise 3.2.3 asdf""]


</div>
</div>","```markdown
# Exercise Node: Coin Flip

## Attributes

- **Name**: Coin Flip

- **Problem Statement**: Consider a single flip of a fair coin. Determine the probability distribution of this experiment using a Bernoulli distribution. Calculate the expected value and variance for the distribution.

  - **Notation**: ""$X \sim \text{Bernoulli}(p)$"" denotes the Bernoulli distribution, where $X$ represents the outcome of the coin flip.

- **Solution**: 

  For a fair coin flip, the probability of getting heads (success) is $p = \frac{1}{2}$. The Bernoulli distribution for this experiment is:

  $$p_X(x) = \begin{cases}
      1-p = \frac{1}{2} \quad \text{if $x=0$ (failure)}\\
      p = \frac{1}{2} \quad \text{if $x=1$ (success)}
  \end{cases}$$

  Using the parameters associated with the Bernoulli distribution:

  - **Expected Value**: The expected value $E(X)$ is given by $E(X) = p = \frac{1}{2}$.

  - **Variance**: The variance $V(X)$ is given by $V(X) = p(1-p) = \frac{1}{2}(\frac{1}{2}) = \frac{1}{4}$.

  - **Step-by-Step Explanation**:
    - Step 1: Recognize the coin flip as a Bernoulli trial with $p = \frac{1}{2}$.
    - Step 2: Use the Bernoulli distribution probability mass function formula.
    - Step 3: Calculate the expected value $E(X) = p$.
    - Step 4: Calculate the variance $V(X) = p(1-p)$.

- **Hints**:
  - Recall that a fair coin implies that the probability of success and failure is equal.
  - Use the Bernoulli distribution formula to determine the probability mass function.

*Knowledge*: *Refer to Bernoulli Distribution section in discrete random variables.*

**Difficulty Level**: Beginner

## Relationships (Edges)

- **has_exercise** <= **is_exercise_for**: *Concept* [Bernoulli Distribution]
- **involved_in_exercise** <= **involves_xxx**: *Concept* [Discrete Random Variable]
  
**SOURCE**: Stat400 teaching material - Jonathan Fernandez
```"
1734098680.04641,Chapter03,context2node-md,Multi-round Coin Flip,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Multi-round Coin Flip</div>
  <div class=""node-type"">exercise</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Exercise Node Schema

> 10/07 No need to look into this in detail for the moment;



## Attributes

- **Name**: *String*
  - A brief title or identifier for the exercise.
  - *Example*: ""Exercise 3.2.1 Poisson Distribution Shopper Passing Through Store Front""

- **Problem Statement**: *Text*
  - The text of the exercise or problem.
  - *Example*: ""Given events A and B, where P(A) = 0.3 and P(B) = 0.5, and P(A ∩ B) = 0.15, compute P(A|B).""
  - **Notation**: *String/Text* (LaTeX)
    - Mathematical notation used in the problem. Show LaTeX code and rendered LaTeX side by side.
    - *Example*: ""P(A ∩ B)"" $P(A \cap B)$
  - **Diagram/Illustration**: *Image*
    - Any visual aids that accompany the problem.
    - *Example*: Venn diagram illustrating events A and B.

- **Solution**: *Text*
  - A detailed solution to the exercise.
  - *Example*: ""Using the definition of conditional probability, \( P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0.15}{0.5} = 0.3 \).""
  - **Step-by-Step Explanation**: *List of Text*
    - Detailed steps explaining how the solution is obtained.
    - *Example*: [""Compute \( P(A \cap B) \)"", ""Use the formula for conditional probability"", ""Simplify the expression""]

- **Hints**: *List of Text* (optional)
  - Hints or guidance to help solve the exercise.
  - *Example*: [""Recall the definition of conditional probability"", ""Ensure that all probabilities are within [0,1]""]

- **Difficulty Level**: *String* (optional)
  - An indication of the difficulty level.
  - *Example*: ""Beginner""

- **Knowledge** (optional)
  - Specific or rare knowledge about the exercise that doesn't fit into other sections.
  - Will be represented in terms of properties with relation links.
  - This part might need further specification after labeling more nodes.

> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships (Edges)

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem*/*Property*/*Application Example* [list of concept/theorem/property/application names] 
  - A list of all nodes that the exercise is about. Concepts, theorems, properties, or application examples that the exercise is about.
  - *Example*: [""Conditional Probability"",]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*/*Theorem*/*Property* [list of concept/theorem/property names]
  - Involves concept/theorem/property, but not as built as an exercise for it.
  - *Example*: [""Joint Probability"", ""Marginal Probability"", ""Bayes' Theorem"", ""Non-negativity""]

- **related_to** <=> *Exercise* [list of exercise names] (optional)
  - Other exercises that are related. For helping student get similar (but different) exercise.
  - *Example*: [""Exercise 3.2.2 asdf"", ""Exercise 3.2.3 asdf""]


</div>
</div>","```markdown
# Exercise Node

## Attributes

- **Name**: Multi-round Coin Flip

- **Problem Statement**: Consider an experiment involving multiple rounds of coin flipping. In each round, a fair coin is flipped until a head is obtained. The number of tails preceding the first head in each round forms our data of interest. Assume that the number of rounds is fixed, say \( n \), and all coin flips are independent. Determine the distribution and parameters associated with the total number of tails observed over all rounds.

  - **Notation**:
    - Let \( X_i \) be the number of tails observed in the \( i \)-th round before the first head.
    - Total number of tails after \( n \) rounds is denoted \( X = X_1 + X_2 + \cdots + X_n \).
    - The distribution of \( X_i \) is \( \text{Geom}(p) \) with \( p = 0.5 \).

- **Solution**:
  - The Bernoulli distribution models a single trial, which is a single coin flip here, with success being getting a head.
  - The Geometric distribution models the random variable \( X_i \), which counts the number of tails before a head.
  - Variance and expected value calculations for \( X_i \).
  
  - **Step-by-Step Explanation**:
    1. Identify each round as a geometric random variable \( X_i \sim \text{Geom}(0.5) \).
    2. Total number of tails, \( X = X_1 + X_2 + \cdots + X_n \), follows a Negative Binomial distribution because it is the sum of independent geometric random variables with the same parameter.
    3. Use properties of expectation \( E(X) = n \times E(X_i) = n \times \frac{1-p}{p} = n \).
    4. Use properties of variance \( V(X) = n \times V(X_i) = n \times \frac{1-p}{p^2} = n \times 2 \).

- **Hints**:
  - Consider the number of trials until the first success as geometric.
  - Use the properties of sum of geometric random variables.
  - Recall the formulas for expectation and variance of geometric distribution.

- **Difficulty Level**: Intermediate

- **Knowledge**: N/A

> **SOURCE**: Stat400 teaching material - Jonathan Fernandez

## Relationships

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*: [""Geometric Distribution"", ""Negative Binomial Distribution""]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*: [""Bernoulli Distribution""]

- **related_to** <=> *Exercise*: N/A
```"
1734098732.4973247,Chapter03,context2node-md,Fair Die Toss,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Fair Die Toss</div>
  <div class=""node-type"">exercise</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Exercise Node Schema

> 10/07 No need to look into this in detail for the moment;



## Attributes

- **Name**: *String*
  - A brief title or identifier for the exercise.
  - *Example*: ""Exercise 3.2.1 Poisson Distribution Shopper Passing Through Store Front""

- **Problem Statement**: *Text*
  - The text of the exercise or problem.
  - *Example*: ""Given events A and B, where P(A) = 0.3 and P(B) = 0.5, and P(A ∩ B) = 0.15, compute P(A|B).""
  - **Notation**: *String/Text* (LaTeX)
    - Mathematical notation used in the problem. Show LaTeX code and rendered LaTeX side by side.
    - *Example*: ""P(A ∩ B)"" $P(A \cap B)$
  - **Diagram/Illustration**: *Image*
    - Any visual aids that accompany the problem.
    - *Example*: Venn diagram illustrating events A and B.

- **Solution**: *Text*
  - A detailed solution to the exercise.
  - *Example*: ""Using the definition of conditional probability, \( P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0.15}{0.5} = 0.3 \).""
  - **Step-by-Step Explanation**: *List of Text*
    - Detailed steps explaining how the solution is obtained.
    - *Example*: [""Compute \( P(A \cap B) \)"", ""Use the formula for conditional probability"", ""Simplify the expression""]

- **Hints**: *List of Text* (optional)
  - Hints or guidance to help solve the exercise.
  - *Example*: [""Recall the definition of conditional probability"", ""Ensure that all probabilities are within [0,1]""]

- **Difficulty Level**: *String* (optional)
  - An indication of the difficulty level.
  - *Example*: ""Beginner""

- **Knowledge** (optional)
  - Specific or rare knowledge about the exercise that doesn't fit into other sections.
  - Will be represented in terms of properties with relation links.
  - This part might need further specification after labeling more nodes.

> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships (Edges)

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem*/*Property*/*Application Example* [list of concept/theorem/property/application names] 
  - A list of all nodes that the exercise is about. Concepts, theorems, properties, or application examples that the exercise is about.
  - *Example*: [""Conditional Probability"",]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*/*Theorem*/*Property* [list of concept/theorem/property names]
  - Involves concept/theorem/property, but not as built as an exercise for it.
  - *Example*: [""Joint Probability"", ""Marginal Probability"", ""Bayes' Theorem"", ""Non-negativity""]

- **related_to** <=> *Exercise* [list of exercise names] (optional)
  - Other exercises that are related. For helping student get similar (but different) exercise.
  - *Example*: [""Exercise 3.2.2 asdf"", ""Exercise 3.2.3 asdf""]


</div>
</div>","```markdown
# Fair Die Toss

## Attributes

- **Name**: Fair Die Toss

- **Problem Statement**: 
  Consider a fair die with six faces numbered from 1 to 6. You are to toss the die once. Determine the probability that you get an odd number.

  - **Notation**: 
    \( P(\text{odd}) \) $P(\text{odd}) = \frac{3}{6} = \frac{1}{2}$

- **Solution**: 
  Since the die is fair, each face has an equal probability of \( \frac{1}{6} \). The numbers that are odd are 1, 3, and 5. Hence, there are three favorable outcomes out of six possible outcomes.

  - **Step-by-Step Explanation**:
    1. Identify the set of outcomes that are odd: \(\{1, 3, 5\}\).
    2. Count the number of favorable outcomes: 3.
    3. Calculate probability: \( P(\text{odd}) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}} = \frac{3}{6} = \frac{1}{2} \).

- **Hints**: 
  - Recall that a fair die means all outcomes are equally likely.
  - Consider only the outcomes that meet the condition of being odd.

- **Difficulty Level**: Beginner

- **Knowledge**: 
  *A fair die has equal likelihood for each outcome.*

## Relationships (Edges)

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem* 
  - Uniform Discrete Random Variable

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*
  - Probability Mass Function

> SOURCE: Stat400 teaching material - Jonathan Fernandez
```
"
1734098815.1680906,Chapter03,context2node-md,Defective Widgets from Production Line,"You will be given the content of a chapter in a book for statistics, a schema for the respective node type, a list of existing `node type;node name` pairs in the graph, and the specific node name. Based on this information, you should generate a markdown-formatted node for the given `node-name` according to the node-type-schema provided.

- Use the content provided to fill out the schema on {topic}.
- For any relevant information that isn't part of the context but is important for the schema, add it in italics (*additional information*).
- You stick to the mathematical notation provided in context as much as possible if it is available.
- Leave blank any parts of the schema that are neither covered in the context nor have additional important information.
- Ensure that the output follows the markdown format for the node, structured in a way that will be useful for later conversion into a Neo4j-compatible format.
- If some information doesn't fit into the schema or doesn't correspond to the node, note it down separately in a markdown file.

Guidelines regarding relationships:
- all relationships except the ones marked with ""MUST"" are optional.
- When Creating Relationship, refer to the existing nodes in the graph, and use the node name from existing nodes list.
- You should keep the all ""(i.e. )"" of relation name from the other direction for relationship that exist for the node.

Input will be provided in following format:
<div class=""input"">
  <div class=""existing-nodes"">{existing_nodes}</div>
  <div class=""node-name"">{node_name}</div>
  <div class=""node-type"">{node_type}</div>
  <div class=""chapter-content"">{chapter_content}</div>
  <div class=""additional-context"">{additional_context}</div>
  <div class=""node-type-schema"">{node_type_schema}</div>
</div>","
<div class=""input"">
  <div class=""existing-nodes"">Empty DataFrame
Columns: [node_type, node_name]
Index: []</div>
  <div class=""node-name"">Defective Widgets from Production Line</div>
  <div class=""node-type"">exercise</div>
  <div class=""chapter-content"">\chapter[Discrete Random Variables]{Discrete Random Variable}
In this chapter we will go over some standard examples of discrete random variables, some of these distributions are so important they have their own specific names. 
\\

Recall that $X$ is said to be a discrete random variables if the cumulative distribution function $F_X$ is a step function. This implies that the set of values of $X$, $\X$, will be a discrete subset of $\R$ (a set with no limit points).

\section[Uniform Discrete Random Variable]{Uniform Discrete Random Variable}
\begin{defn}
The random variable $X$ has \textbf{uniform discrete distribution} with parameter $N\in \N$ if it is supported on

$$\X:=\{1,2,3,\cdots,N\}$$
and probability mass function of $X$ is given by:

$$p_X(x)=P(X=x)=\frac{1}{N} \quad \quad \text{for all $x\in \X$}$$
\end{defn}

\begin{thm}[Parameters associated to the Uniform Distribution]
    Suppose $X$ has the Uniform distribution on $\{1, 2, 3, \dots, N\}$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{N+1}{2}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{(N+1)(N-1)}{12}$$
    \end{enumerate}
\end{thm}

\section[Bernoulli Distribution]{Bernoulli Distribution}
The Bernoulli distribution simulates a ""trial"" whose outcome is either a success or a failure. 

\begin{defn}[Bernoulli Trial]
An experiment is called a Bernoulli trial if it has exactly two outcomes, a success or a failure. 
\end{defn}

\begin{defn}[Bernoulli Distribution]
    We say that $X$ has the Bernoulli distribution with parameter $p\in (0,1)$, typically denoted by $X \sim \text{Bernoulli}(p)$ if $X$ takes values $\mathcal{X}:= \{0,1\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \begin{cases}
        1-p \quad \text{if $x=0$}\\
        p \quad \text{if $x=1$}
    \end{cases}$$
    The pmf can also be written as
    $$p_X(x) = p^x(1-p)^{(1-x)}\quad \quad x\in \{0,1\}$$
\end{defn}

Events associated to the Bernoulli distribution are of the form:
\begin{enumerate}
    \item $\{X = 0\}$: the trial was a failure.
    \item $\{X =1\}$: the trial was a success. 
\end{enumerate}


\begin{thm}[Parameters associated to the Bernoulli Distribution]
    Suppose $X\sim \text{Bernoulli}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = p$$
        \item The variance of $X$ is given by:
        $$V(X) = p(1-p)$$
    \end{enumerate}
\end{thm}

Examples of Bernoulli distribution in applications:
\begin{enumerate}
    \item Suppose we sample a random student from campus and check if the height of the student is less than or equal to 60inches. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has height $\le 60$ inches}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of students on campus whose height is less than or equal to 60inches. 
    \item  Suppose we sample a random student from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen student has taken a statistics course}\}$$
    \\
    In this setting, $p_X(0) = P(X=0) = (1-p)$, is the proportion of students on campus who have never taken a statistics class. 
    \item  Suppose we choose a potato at a supermarket, and we put it into our cart if it has no blemishes.  
    \\
    This experiment can be modelled by $X\sim \text{Bernoulli}(p)$ distribution, where the event 
    $$\{X=1\} := \{\text{a randomly chosen potato has no blemishes}\}$$
    \\
    In this setting, $p_X(1) = P(X=1) = p$, is the proportion of potatoes without blemishes in the supermarket aisle. 
\end{enumerate}

\section[Binomial Distribution]{Binomial Distribution}
Suppose an experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. An outcome for this experiment is a sequence of length $n$, with the $i$th entry in the sequence tracking if the $i$th trial was a success or a failure. In this setting, we might be interested in knowing the number of success in the outcome, and the Binomial random variable does exactly that. 
\\

\begin{defn}[Binomial Distribution]
    We say that $X$ has the Binomial distribution with parameters
    \begin{itemize}
        \item $n\in \mathbb{N}$ : the number of trials
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Binomial}(n, p)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, n\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {n \choose x} p^x(1-p)^{(n-x)}\quad \quad x\in \{0,1, 2, 3, \dots, n\}$$
\end{defn}

\begin{thm}[Binomial Experiment]
    Suppose $n\in \mathbb{N}$. The experiment involves performing $n$ independent $\text{Bernoulli}(p)$ trials. 
    \\
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of successes in $\omega$}$$
    Then, 
    $$X \sim \text{Binomial}(n, p)$$
\end{thm}
\begin{proof}
The proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Binomial distribution with the given parameters. We leave this as an exercise for the reader. 
\end{proof}


Events associated to the Binomial distribution are of the form:
\begin{enumerate}
    \item $\{X = k \}$: there are exactly $k$ successes in $n$ trials. 
    \item $\{a < X < b\}$: the number of successes in $n$ trials are greater  than $a$ but less than $b$.
    \item $\{a \le X\}$: there are more than $a$ successes in $n$ trials.
\end{enumerate}


\begin{thm}[Parameters associated to the Binomial Distribution]
    Suppose $X\sim \text{Binomial}(n, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = np$$
        \item The variance of $X$ is given by:
        $$V(X) = np(1-p)$$
    \end{enumerate}
\end{thm}
Examples of Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample $n$ students (with replacement) from campus and check if their height of the chosen student is less than or equal to 60inches. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X= k \} := \{\text{there are exactly $k$ students with height $\le 60$ inches among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \\
    Also, $P(X \ge k)$, is probability of finding at least  $k$ students with height $\le 60$ among $n$ randomly chosen students. 
    \item  Suppose we sample $n$ students (with replacement) from campus and check if they have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ students who have taken a statistics class among the $n$ chosen students}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability that of finding exactly $k$ students who have taken a statistics class among the $n$ chosen students. 
    
    \item  Suppose we choose $n$ potatoes (with replacement, even though that does not really make sense), a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Binom}(n, p)$ distribution, where the event 
    $$\{X=k\} := \{\text{there are exactly $k$ good potatoes among $n$ randomly chosen potatoes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes without blemishes among $n$ randomly chosen potatoes. 
\end{enumerate}
Note that we have be emphasizing that the sampling be with replacement, this ensures that trials are independent of each other.
\\

In many applications, where the population is large and $n$ is small (in comparison to the population), sampling without replacement can still result in ""approximately"" independent trials.
\\

When we are sampling from a population without replacement, the random variable that tracks the number of successes actually has the ""Hypergeometric Distribution""

\section{Hypergeometric Distribution}
Fix $N,M,n\in \N$. Suppose we have population of size $N$ with exactly $M$ successes in it. Suppose an experiment involves choosing $n$ objects from the population (without replacement). Given $\omega$ an outcome for this experiment, let
$$X(\omega) := \text{number of successes in $\omega$}$$, in this setting $X$ is said to have the H/ypergeometeric distribution.


\begin{defn}[Hypergeometric Distribution]
    We say that $X$ has the Hypergeometric distribution with parameters
    \begin{itemize}
        \item $N\in \mathbb{N}$ : population size
        \item $M \in \mathbb{N}$: number of success in the population. 
        \item $n\in \mathbb{N}$: number individuals chosen from the population. 
    \end{itemize}
    typically denoted by $X \sim \text{Hyper}(N, M, n)$ if $X$ takes values $\mathcal{X}:= \{0,1, 2, 3, \dots, \min(n, M)\}$ and the probability mass function of $X$ is given as
    $$p_X(x) = \frac{{N \choose x} {N-M \choose n-x}}{{N \choose n}}\quad \quad x\in \{0,1, 2, 3, \dots, \min(n, M)\}$$
\end{defn}

\begin{thm}[Parameters associated to the Hypergeometric Distribution]
    Suppose $X\sim \text{Hyper}(N, M, n)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = n \left(\frac{M}{N}\right)$$
        \item The variance of $X$ is given by:
        $$V(X) = \left(\frac{N-n}{N-1} \right)\cdot n\cdot \left(\frac{M}{N}\right)\cdot \left(1-\frac{M}{N} \right) $$
    \end{enumerate}
\end{thm}

\section{Geometric Distribution}
The geometric random variable models phenomena that involve ""waiting"". 

\begin{defn}[Geometric Distribution]
    We say that $X$ has the Geometric distribution with parameters
    \begin{itemize}
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{Geom}(p)$ if $X$ takes values $\mathcal{X}:= \{1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = p^x(1-p)^{(x-1)}\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Geometric Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until the first success is observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of trials in $\omega$}$$
    Then, 
    $$X \sim \text{Geom}(p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment looks like:
$$ \mathcal{S} = \{S, FS, FFS, FFFS, FFFFS, \dots, \}$$
From here, the proof involves explicitly calculating the pmf of $X$ and showing that it is the same as the pmf of the Geometric distribution with parameter $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Geometric distribution models phenomena that involve waiting for one success. 

\begin{thm}[Parameters associated to the Geometric Distribution]
    Suppose $X\sim \text{Geom}(p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{1}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{1-p}{p^2}$$
    \end{enumerate}
\end{thm}
Examples of Geometric distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until a student with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students before the first student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of picking exactly $k$ students before finding a student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, picking more than or equal to $k$ students before finding a student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find one that has taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we pick $k$ students before finding the first student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of picking exactly $k$ students before finding a student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find a potato that does not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{Geom}(p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{we pick $k$ potatoes before finding the first potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of picking exactly $k$  potatoes before finding the first potato without any blemishes. 
\end{enumerate}


\section{Negative Binomial Distribution}
The Negative Binomial random variable also models phenomena that involve ""waiting"". 

\begin{defn}[Negative Binomial Distribution]
    We say that $X$ has the Negative Binomial distribution with parameters
    \begin{itemize}
    \item $r \in \mathbb{N}$: number of successes we are waiting for.
        \item $p\in (0,1)$: $P(\text{Success})$, the probability of a success. 
    \end{itemize}
    typically denoted by $X \sim \text{NegBinom}(r, p)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = {x+r-1 \choose x}p^r(1-p)^x\quad \quad x\in \{0,1, 2, 3, \dots, \}$$
\end{defn}


\begin{thm}[Negative Binomial Experiment]
    The experiment involves performing independent $\text{Bernoulli}(p)$ trials until $r$ successes are observed.  
    \\  
    Let $\omega$ be an outcome for this experiment, and $$X(\omega) := \text{number of failures in $\omega$}$$
    Then, 
    $$X \sim \text{NegBinom}(r, p)$$
\end{thm}
\begin{proof}
We first observe that the sample space for this experiment is infinite, and looks like
$$ \mathcal{S} = \{SS\dots S, FSS\dots S, SFS\dots S, \dots , \}$$

The proof involves explicitly counting the outcomes in the sets 
$$\{X = k\} = \{\text{exactly $k$ failures observed before the $r$th success}\}$$ 
and using this to calculate the pmf of $X$, and showing that it is the same as the pmf of the Negative Binomial distribution with parameters $r$ and $p$. We leave this as an exercise for the reader. 
\end{proof}
The above theorem shows that the Negative Binomial distribution models phenomena that involve waiting for $r$ successes. 


\begin{thm}[Parameters associated to the Negative Binomial Distribution]
    Suppose $X\sim \text{NegBinom}(r, p)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \frac{r(1-p)}{p}$$
        \item The variance of $X$ is given by:
        $$V(X) = \frac{r(1-p)}{p^2}$$
    \end{enumerate}
\end{thm}


Examples of Negative Binomial distribution in applications:
\begin{enumerate}
    \item Suppose we sample students (with replacement) from campus until we find $r$ students with height is less than or equal to 60inches is found. 
    \\
    Since the sampling is with replacement, the result of a given trial is independent of the other trials. Therefore this experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, where $$P(\text{Success}) = P(\text{finding a student with height less than or equal to 60 inchex})$$ 
    and  event
    $$\{X= k \} := \{\text{we find $k$ students of height greater than 60inches before the finding the $r$th student with height less than or equal to 60 inches}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is probability of finding exactly $k$ students with height greater than 60inches before finding the $r$th student with height less than or equal to 60inches.  
    \\
    Also, $P(X \ge k)$, finding more than or equal to $k$ students with height greater than 60inches before finding $r$th student with height less than or equal to 60inches.
    \item  Suppose we sample students (with replacement) from campus until we find $r$ students that have taken a statistics course. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    where
    $$P(\text{Success}) = P(\text{finding a student who has taken a statistics course})$$
    and the event 
    $$\{X= k \} := \{\text{we find $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$,  is the probability of finding exactly $k$ students who have never taken a statistics class before finding $r$th student who has taken at least one statistics class. 
    
    \item  Suppose we choose potatoes (with replacement, even though that does not really make sense) until we find $r$ potatoes that do not have a blemish. 
    \\
    
    In this setting, a potato is a success if it does not have any blemishes. 
    \\
    This experiment can be modelled by $X\sim \text{NegBinom}(r, p)$ distribution, 
    $$P(\text{Success}) = P(\text{finding a potato without any blemishes})$$
    
    and the event 
    $$\{X=k\} := \{\text{find exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes}\}$$
    \\
    In this setting, $p_X(k) = P(X=k)$, is the probability of finding exactly $k$ potatoes with blemishes before finding the $r$th potato without any blemishes. 
\end{enumerate}


\section{Poisson Distribution}
The Poisson is distribution is a very useful distribution that shows up in many applications, especially in the context of Poisson Processes. 

\begin{defn}[Poisson Distribution]
    We say that $X$ has the Poisson  distribution with parameters
    \begin{itemize}
    \item $\lambda \in \mathbb{R}_{+}$: rate parameter
    \end{itemize}
    typically denoted by $X \sim \text{Pois}(\lambda)$ if $X$ takes values $\mathcal{X}:= \{0, 1, 2, 3, \dots, \} = \mathbb{N}$ and the probability mass function of $X$ is given as
    $$p_X(x) = e^{-\lambda} \frac{\lambda^k}{k!}$$
\end{defn}

To show that $p_X$ is indeed a pmf, we observe the series expansion 
$$e^{\lambda}= \sum_{k=0}^\infty \frac{\lambda^k}{k!}$$

\begin{thm}[Parameters associated to the Poisson Distribution]
    Suppose $X\sim \text{Pois}(\lambda)$ then,
    \begin{enumerate}
        \item The expected value of $X$ is given by:
        $$E(X) = \lambda$$
        \item The variance of $X$ is given by:
        $$V(X) = \lambda$$
    \end{enumerate}
\end{thm}



\section[Relationships Between Discrete Distributions]{Relationships Between Discrete Distributions}
\subsection[Binomial and Hypergeometric]{Binomial and Hypergeometric}
Let $N,M,n\in \N$ be given and set $p=\frac{M}{N}$. Suppose,
\begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item hyper$(x;N,M,n)$ is the probability mass function for hypergeometric distribution with parameters\begin{align*}
n&=\text{ sample size}\\
M&=\text{number of successes}\\
N&=\text{population size}
\end{align*}
Then, if $N,M\to \infty$, and $\frac{M}{N}\to p$, then hyper$(x;N,M,n)\to$bin$(x;n,p)$. 
\\

An alternate way to state this is to say that if sample size N is small compared to population size $N$, we can assume that samples are approximately independent.
\end{enumerate}
\subsection[Binomial and Poisson]{Binomial and Poisson}
Suppose \begin{enumerate}[itemsep=0pt, topsep=1pt, partopsep=0pt,label=(\alph*)]
\item bin$(x;n,p)$ is the probability mas function for binomial Distribution with parameters: \begin{align*}
n&=\text{ sample size}\\
p&=\text{ probability of success}
\end{align*}
\item pois$(x;\lambda)$ is the probability mass function for Poisson Distribution with parameters $\lambda>-$
\end{enumerate}
Then, if $n\to \infty$ and $p\to 0$ such that $n\cdot p\to \lambda$, then then bin$(x;N,M,n)\to$ pois$(x;n,p)$. 
\\

If $n$ is large and $p$ is small, bin$(x;N,M,n)\approx$ pois$(x;n,p)$, this is sometimes stated by saying that the Poisson Distribution is approximately the Binomial Distribution for rare events.</div>
  <div class=""additional-context""></div>
  <div class=""node-type-schema""># Exercise Node Schema

> 10/07 No need to look into this in detail for the moment;



## Attributes

- **Name**: *String*
  - A brief title or identifier for the exercise.
  - *Example*: ""Exercise 3.2.1 Poisson Distribution Shopper Passing Through Store Front""

- **Problem Statement**: *Text*
  - The text of the exercise or problem.
  - *Example*: ""Given events A and B, where P(A) = 0.3 and P(B) = 0.5, and P(A ∩ B) = 0.15, compute P(A|B).""
  - **Notation**: *String/Text* (LaTeX)
    - Mathematical notation used in the problem. Show LaTeX code and rendered LaTeX side by side.
    - *Example*: ""P(A ∩ B)"" $P(A \cap B)$
  - **Diagram/Illustration**: *Image*
    - Any visual aids that accompany the problem.
    - *Example*: Venn diagram illustrating events A and B.

- **Solution**: *Text*
  - A detailed solution to the exercise.
  - *Example*: ""Using the definition of conditional probability, \( P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{0.15}{0.5} = 0.3 \).""
  - **Step-by-Step Explanation**: *List of Text*
    - Detailed steps explaining how the solution is obtained.
    - *Example*: [""Compute \( P(A \cap B) \)"", ""Use the formula for conditional probability"", ""Simplify the expression""]

- **Hints**: *List of Text* (optional)
  - Hints or guidance to help solve the exercise.
  - *Example*: [""Recall the definition of conditional probability"", ""Ensure that all probabilities are within [0,1]""]

- **Difficulty Level**: *String* (optional)
  - An indication of the difficulty level.
  - *Example*: ""Beginner""

- **Knowledge** (optional)
  - Specific or rare knowledge about the exercise that doesn't fit into other sections.
  - Will be represented in terms of properties with relation links.
  - This part might need further specification after labeling more nodes.

> NEED TO mark SOURCE for the information used here. By default we will use `Stat400 teaching material - Jonathan Fernandez`

## Relationships (Edges)

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem*/*Property*/*Application Example* [list of concept/theorem/property/application names] 
  - A list of all nodes that the exercise is about. Concepts, theorems, properties, or application examples that the exercise is about.
  - *Example*: [""Conditional Probability"",]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*/*Theorem*/*Property* [list of concept/theorem/property names]
  - Involves concept/theorem/property, but not as built as an exercise for it.
  - *Example*: [""Joint Probability"", ""Marginal Probability"", ""Bayes' Theorem"", ""Non-negativity""]

- **related_to** <=> *Exercise* [list of exercise names] (optional)
  - Other exercises that are related. For helping student get similar (but different) exercise.
  - *Example*: [""Exercise 3.2.2 asdf"", ""Exercise 3.2.3 asdf""]


</div>
</div>","```markdown
# Exercise Node

## Attributes

- **Name**: Defective Widgets from Production Line

- **Problem Statement**: 
  A production line produces widgets. The probability of a widget being defective is 0.05. Assuming independence, and that 20 widgets are checked, what is the probability that exactly 2 widgets are defective? What is the variance and expected value of the number of defective widgets?

  - **Notation**:
    - ""Probability of a widget being defective: \( p = 0.05 \)""
    - ""Number of widgets checked: \( n = 20 \)""
    - ""Probability of exactly 2 defective widgets, \( P(X = 2) \), where \( X \sim \text{Binomial}(n = 20, p = 0.05) \)""
    - ""Expected value, \( E(X) = np \)""
    - ""Variance, \( V(X) = np(1-p) \)""

- **Solution**: 
  To solve this problem, we model the situation using the Binomial distribution with parameters \( n = 20 \) and \( p = 0.05 \).

  **Step-by-Step Explanation**:
  1. Identify the distribution parameters: \( n = 20 \), \( p = 0.05 \).
  2. Calculate the probability of exactly 2 defective widgets using the binomial probability mass function:
     \[
     P(X = 2) = \binom{20}{2} (0.05)^2 (0.95)^{18}
     \]
  3. Compute the expected value and variance:
     \[
     E(X) = np = 20 \times 0.05 = 1
     \]
     \[
     V(X) = np(1-p) = 20 \times 0.05 \times 0.95 = 0.95
     \]

- **Hints**:
  - Recall the Binomial distribution formula \( P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} \).
  - The expected value of a binomially distributed variable is \( E(X) = np \).
  - The variance of a binomially distributed variable is \( V(X) = np(1-p) \).

- **Difficulty Level**: Intermediate

## Relationships

- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept* [Binomial Distribution]

- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept* [Probability Mass Function, Expected Value, Variance]
```"
1734098886.3330708,Chapter03,md2json,Geometric Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => Discrete Distribution""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734098911.9821792,Chapter03,md2json,Hypergeometric Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution]""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734098922.2333524,Chapter03,md2json,Negative Binomial Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution] \n  - A parent concept \""Discrete Distribution\"" has current concept \""Negative Binomial Distribution\"" as a subconcept.""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734098931.928656,Chapter03,md2json,Poisson Distribution,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **is_subconcept_of** => (i.e. **has_subconcept** <=) *Concept* [Discrete Distribution]\n  - Explanation: The Poisson Distribution is a type of discrete random variable distribution used to model the probability of a number of events occurring within a specific timeframe.\n""}","{""relationships"":{""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_name"":""Discrete Distribution"",""target_node_type"":""concept""}}],""direction"":""from""},""is_prerequisite_of"":null}}"
1734098989.2445152,Chapter03,md2json,Coin Flip,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": "" (Edges)\n\n- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept* [Bernoulli Distribution]\n- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept* [Discrete Random Variable]\n  \n**SOURCE**: Stat400 teaching material - Jonathan Fernandez""}","{""relationships"":{""has_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Bernoulli Distribution""}}],""direction"":""from""},""involved_in_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Discrete Random Variable""}}],""direction"":""to""},""related_to"":null}}"
1734099364.3413427,Chapter03,md2json,Defective Widgets from Production Line,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept* [Binomial Distribution]\n\n- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept* [Probability Mass Function, Expected Value, Variance]""}","{""relationships"":{""has_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Binomial Distribution""}}],""direction"":""from""},""involved_in_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function""}},{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value""}},{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance""}}],""direction"":""from""},""related_to"":null}}"
1734099376.5221703,Chapter03,md2json,Fair Die Toss,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": "" (Edges)\n\n- **has_exercise** <= (i.e. **is_exercise_for** =>) *Concept*/*Theorem* \n  - Discrete Uniform Random Variable\n\n- **involved_in_exercise** <= (i.e. **involves_xxx** =>) *Concept*\n  - Probability Mass Function\n\n> SOURCE: Stat400 teaching material - Jonathan Fernandez""}","{""relationships"":{""has_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Discrete Uniform Random Variable""}}],""direction"":""from""},""involved_in_exercise"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function""}}],""direction"":""from""},""related_to"":null}}"
1734099399.9703915,Chapter03,md2json,Expected value of Geometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*: Geometric Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*: Expected Value of a Random Variable\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*:  \n  *(No subproperties specified)*\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*:  \n  *(No specific dependencies provided)*""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Geometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of a Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099430.847278,Chapter03,md2json,Expected value of Hypergeometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST\n  - Hypergeometric Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST\n  - Expected Value of Random Variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Hypergeometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099442.1040292,Chapter03,md2json,Expected value of Negative Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Negative Binomial Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Expected Value of Random Variable\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n  - \n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n  -""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Negative Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected Value of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099451.066157,Chapter03,md2json,Expected value of Poisson Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Poisson Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Expected value of random variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Poisson Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Expected value of random variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099459.2072284,Chapter03,md2json,PMF of Geometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept* MUST\n  - Geometric Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* MUST\n  - Probability Mass Function (PMF)\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Geometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099481.3282075,Chapter03,md2json,PMF of Hypergeometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*: Hypergeometric Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*: Probability Mass Function (PMF)\n- **has_subproperty** => (i.e. **is_subproperty_of** <=) *Property*: N/A\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*: N/A\n\n---\n\n*Additional Notes:* \n- The PMF of the Hypergeometric Distribution is crucial for understanding events that involve sampling without replacement.\n- This conceptual understanding also highlights the differences from similar discrete distributions like the Binomial Distribution, which assumes replacement.""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Hypergeometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099489.5877917,Chapter03,md2json,PMF of Negative Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept* Negative Binomial Distribution\n  - The **Negative Binomial Distribution** has_property the current property.\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept* Probability Mass Function (PMF)\n  - The current property \""PMF of Negative Binomial Distribution\"" is a subconcept of \""Probability Mass Function\""; \""Probability Mass Function\"" has_subconcept the current property.""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Negative Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099497.3884225,Chapter03,md2json,PMF of Poisson Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Poisson Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Probability Mass Function (PMF)""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Poisson Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Probability Mass Function (PMF)""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099504.5743575,Chapter03,md2json,Variance of Geometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) \n  - *Geometric Distribution*\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) \n  - *Variance of Random Variable*\n\n- **has_subproperty** => (i.e. **is_subproperty_of** <=)\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) \n  - *Expected Value of Geometric Distribution*\n\n## Additional Information\n\n- **Source**: Information pertaining to the variance of the geometric distribution is based on standard statistical teaching materials like `Stat400 teaching material - Jonathan Fernandez`. \n\n*Note*: The proof generally requires showing that the variance formula arises from the geometric series calculations for expectations in the geometric distribution context.""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Geometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":{""target_nodes"":[{""target_node"":{""target_node_type"":""property"",""target_node_name"":""Expected Value of Geometric Distribution""}}],""direction"":""from""},""has_subproperty"":null}}"
1734099548.1614282,Chapter03,md2json,Variance of Hypergeometric Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - **Target**: Hypergeometric Distribution\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - **Target**: Variance of random variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Hypergeometric Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of random variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099562.9243302,Chapter03,md2json,Variance of Negative Binomial Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) Negative Binomial Distribution\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) Variance of random variable""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Negative Binomial Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of random variable""}}],""direction"":""from""},""is_prerequisite_of"":null,""has_subproperty"":null}}"
1734099570.382424,Chapter03,md2json,Variance of Poisson Random Variable,"You will be provided with a markdown node on a statistical knowledge, and a JSON schema for the respective node type. Your task is to convert the markdown node to a JSON object according to the schema.
Note that you should not change the content corresponding to each subsection in the markdown node; and you should use the formatting for latex in markdown, that is, using $...$ for inline math and $$...$$ for display math.","{""markdown_node"": ""\n\n- **has_property** <= (i.e. **is_property_of** =>) *Concept*\n  - Concept: \""Poisson Distribution\""; \""Poisson Distribution\"" has_property current property.\n\n- **has_subconcept** <= (i.e. **is_subconcept_of** =>) *Concept*\n  - Concept: \""Variance of Random Variable\""; \""Variance of Random Variable\"" has_subconcept current property.\n\n- **is_prerequisite_of** <= (i.e. **depends_on** =>) *Property*\n  - Property: \""Expected Value of Poisson Distribution\""; \""Expected Value of Poisson Distribution\"" is a prerequisite of current property.""}","{""relationships"":{""has_property"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Poisson Distribution""}}],""direction"":""from""},""has_subconcept"":{""target_nodes"":[{""target_node"":{""target_node_type"":""concept"",""target_node_name"":""Variance of Random Variable""}}],""direction"":""from""},""is_prerequisite_of"":{""target_nodes"":[{""target_node"":{""target_node_type"":""property"",""target_node_name"":""Expected Value of Poisson Distribution""}}],""direction"":""from""},""has_subproperty"":null}}"
